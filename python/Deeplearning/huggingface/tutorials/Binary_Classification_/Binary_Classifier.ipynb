{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cef7d93-6126-4cf9-8d6b-8a52aaa7f67f",
   "metadata": {},
   "source": [
    "# 네이버 영화 리뷰 감성분석\n",
    "\n",
    "최소한의 성능이라도 작동하는 코드를 작성하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2db6161-fa3a-4747-9295-c2c24d83f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch_optimizer as custom_optim\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f89ef8-9c40-4e30-a69f-1217fbff8592",
   "metadata": {},
   "source": [
    "## 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be537c3-132a-4a5e-b4e5-fc6d61abe675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kyeul\\anaconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "nsmc_dataset = load_dataset('nsmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8afbf14a-af5d-4374-b3a8-de3b0599e691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796c0f5-d9d1-4ec4-a58a-e036c4ae358a",
   "metadata": {},
   "source": [
    "간단한 EDA를 통해 데이터 분포와 특징을 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95420984-4b86-4bf3-9aa4-b9a3fd05c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data move to dataFrame\n",
    "nsmc_df = nsmc_dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf04581b-d452-4d0c-894f-8ebf0e3648d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75173</td>\n",
       "      <td>75173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74827</td>\n",
       "      <td>74827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  document\n",
       "label                 \n",
       "0      75173     75173\n",
       "1      74827     74827"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1c19f-9abb-400f-9d50-21d4b3a295da",
   "metadata": {},
   "source": [
    "label 분포는 적당하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c19128a3-2683-42a1-b38d-f97ed4905d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    150000.000000\n",
       "mean         35.203353\n",
       "std          29.532097\n",
       "min           0.000000\n",
       "25%          16.000000\n",
       "50%          27.000000\n",
       "75%          42.000000\n",
       "max         146.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsmc_df['length'] = nsmc_df['document'].str.len()\n",
    "nsmc_df['length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e56a1a-8748-455a-aaeb-d44606d7fa7f",
   "metadata": {},
   "source": [
    "0-0 base line  \n",
    "리뷰인데, 최솟값이 0인 것이 보인다.   \n",
    "추후에 데이터를 자세히 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0eb8b27-94cc-4dcb-b004-c2da1469e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Aurguement\n",
    "batch_size = 256\n",
    "max_length = 146\n",
    "warmup_ratio = 0.2\n",
    "pretrained_model = \"klue/roberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d0b6fa-a9ba-41b2-9998-c3bda72fa737",
   "metadata": {},
   "source": [
    "## 전처리\n",
    "1. train 데이터를 random하게 shuffleing한다.\n",
    "2. train 데이터를 train과 valid셋으로 나눈다.\n",
    "3. DataLoader에 주입하고 batch 별 데이터에 맞게 collate를 수행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a2781e8-c07d-47da-81dd-464b8f99a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split(data, valid_ratio=.2):\n",
    "    data, labels = data['document'], data['label']\n",
    "\n",
    "    # label-index map을 만듬(해당 task에서는 label이 이미 숫자이므로 수행할 필요는 없지만 \n",
    "    # 범용성을 위해 수행)\n",
    "    unique_labels = list(set(labels))\n",
    "    label_to_index = {}\n",
    "    index_to_label = {}\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        label_to_index[label] = i\n",
    "        index_to_label[i] = label\n",
    "\n",
    "    # label value를 integer value로 convert 수행\n",
    "    labels = list(map(label_to_index.get, labels))\n",
    "\n",
    "    # Shuffle before split into train and validation set\n",
    "    shuffled = list(zip(data, labels))\n",
    "    random.shuffle(shuffled)\n",
    "    data = [element[0] for element in shuffled]\n",
    "    labels = [element[1] for element in shuffled]\n",
    "    idx = int(len(data) * (1 - valid_ratio)) # split할 경계의 index값\n",
    "\n",
    "    data = {\n",
    "\n",
    "        'train':{\n",
    "            'document':data[:idx],\n",
    "            'label':labels[:idx]\n",
    "        },\n",
    "        'validation':{\n",
    "            'document':data[idx:],\n",
    "            'label':labels[:idx]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return data, index_to_label\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e784b3bf-3157-430e-abd9-46d4af6feadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, index_to_label = shuffle_and_split(nsmc_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0829c58b-ae5f-4e09-a825-a3803ea5146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationCollator():\n",
    "    def __init__(self, tokenizer, max_length, with_text=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.with_text = with_text # tokenization을 통과하면 text는 반환하지 않기 때문에 필요에 \n",
    "                                   # 따라 원본 텍스르도 함께 반환하도록 한다. \n",
    "\n",
    "    def __call__(self, samples):\n",
    "        texts, labels = [], []\n",
    "        for text, label in samples:\n",
    "            texts += [text]\n",
    "            labels += [label]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        return_value = {\n",
    "            'input_ids':encoding['input_ids'],\n",
    "            'attention_mask':encoding['attention_mask'],\n",
    "            'labels':torch.tensor(labels, dtype=torch.long),    \n",
    "        }\n",
    "        if self.with_text:\n",
    "            return_value['text'] = texts \n",
    "\n",
    "        return return_value\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbf391b0-31f9-4044-b83f-f04396143559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|████████████████████████████████████████████████████| 375/375 [00:00<?, ?B/s]\n",
      "C:\\Users\\Kyeul\\anaconda3\\envs\\nlp\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Kyeul\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)solve/main/vocab.txt: 100%|███████████████████████████████████████████| 248k/248k [00:00<00:00, 721kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████████████████████████████████████| 752k/752k [00:00<00:00, 29.7MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|████████████████████████████████████████████████████| 173/173 [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ecccaa59-c5db-496b-8240-844bd216f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    TextClassificationDataset(data['train']['document'], data['train']['label']),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=TextClassificationCollator(tokenizer, max_length)\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    TextClassificationDataset(data['validation']['document'], data['validation']['label']),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=TextClassificationCollator(tokenizer, max_length)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "835947f7-da04-4777-b123-01ffa0f98be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120064\n",
      "30208\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader)*batch_size)\n",
    "print(len(valid_loader)*batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df0d8d-c24c-4182-8f63-23b426ebf6c4",
   "metadata": {},
   "source": [
    "## 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a78b5088-d128-46ae-9d34-8c8d2a83c378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14e1c924-8154-412b-90d4-73ada5f72fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b77b7d2-3999-4840-bfd2-007bc314d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyper-parameters\n",
    "epochs = 5\n",
    "learning_rate = 5e-4\n",
    "n_total_iterations = len(train_loader)*batch_size\n",
    "n_warmup_steps = int(n_total_iterations * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b6f024b-80f6-4516-bb07-bd87837e3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set AdamW\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.01\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=learning_rate,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a60da9c6-6ed4-4951-a8d9-6d215b2d1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set criterion\n",
    "crit = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6439839-cc9d-4689-8ead-ed6d677d9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    n_warmup_steps,\n",
    "    n_total_iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3cffe8f6-638a-4812-a95a-594d00ca7153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCELoss()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()\n",
    "crit.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5de1e9-00cb-406b-872b-7cade22c4cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
