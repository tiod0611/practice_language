{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "097f7cb6-4331-4a5c-b6e0-862b884d8df6",
   "metadata": {},
   "source": [
    "### This code from [here](https://pseudo-lab.github.io/klue-baseline/docs/TC-1.html)\n",
    "\n",
    "# HuggingFace Hubë¥¼ í™œìš©í•œ Fine tuning Baseline(YNAT ver.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa688eb-6117-4f2d-9917-3540014da335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyeul\\anaconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb8a67d2-c38a-4ae7-b654-2c1d2a909825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# argment setting\n",
    "task = \"ynat\"\n",
    "# model_checkpoint = \"klue/bert-base\"\n",
    "model_checkpoint = \"klue/roberta-large\"\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572a309-0b01-4547-baa6-9ff309fcc785",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443b8ac9-dce9-4765-9cb3-d0e42df1b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3dee67c-5e3f-450d-83ef-401cd6383ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['guid', 'title', 'label', 'url', 'date'],\n",
       "        num_rows: 45678\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['guid', 'title', 'label', 'url', 'date'],\n",
       "        num_rows: 9107\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('klue', 'ynat')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38bf5b8a-84b4-41f1-a8ec-61d68283e630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'ynat-v1_train_00000',\n",
       " 'title': 'ìœ íŠœë¸Œ ë‚´ë‹¬ 2ì¼ê¹Œì§€ í¬ë¦¬ì—ì´í„° ì§€ì› ê³µê°„ ìš´ì˜',\n",
       " 'label': 3,\n",
       " 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
       " 'date': '2016.06.30. ì˜¤ì „ 10:36'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show sample\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6a12a4-18e2-4dc3-8e64-8dc362680fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_17629</td>\n",
       "      <td>ì—¬ë¦„ íœ´ê°€ì§€ êµ­ë‚´ 1ìœ„ëŠ” ì œì£¼ë„â€¦í•´ì™¸ëŠ” ì¼ë³¸</td>\n",
       "      <td>ìƒí™œë¬¸í™”</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=103&amp;sid2=237&amp;oid=001&amp;aid=0010205449</td>\n",
       "      <td>2018.07.12. ì˜¤ì „ 11:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_10036</td>\n",
       "      <td>ì •ë¶€ ì‡ë‹¨ ê·œì œì—ë„â€¦ì‘ë…„ ì€í–‰ ì£¼íƒëŒ€ì¶œ 15ì¡° ì´ìƒ ëŠ˜ì–´</td>\n",
       "      <td>ê²½ì œ</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=101&amp;sid2=258&amp;oid=001&amp;aid=0009787687</td>\n",
       "      <td>2018.01.03. ì˜¤ì „ 6:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_train_09017</td>\n",
       "      <td>ëŸ°ë˜ ì¶•êµ¬ì¥ì—ì„œ ì—´ë¦¬ëŠ” MLB ë¼ì´ë²Œì „ ì–‘í‚¤ìŠ¤ vs ë ˆë“œì‚­ìŠ¤</td>\n",
       "      <td>ìŠ¤í¬ì¸ </td>\n",
       "      <td>https://sports.news.naver.com/news.nhn?oid=001&amp;aid=0010916493</td>\n",
       "      <td>2019.06.27 10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ynat-v1_train_36822</td>\n",
       "      <td>ë‚˜ë„ ëª¨ë¥´ê²Œ ì¶œë ¥ì´â€¦ í”„ë¦°í„° í•´í‚¹ í”¼í•´ í™•ì‚°</td>\n",
       "      <td>ì‚¬íšŒ</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=105&amp;sid2=230&amp;oid=001&amp;aid=0009011550</td>\n",
       "      <td>2017.02.06. ì˜¤í›„ 3:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_23084</td>\n",
       "      <td>ê²Œì‹œíŒ KT 5G ì•¼êµ¬ ì›¹ì˜ˆëŠ¥ ì˜¤ì§€ëŠ” ì•¼êµ¬ë‹¨ ê³µê°œ</td>\n",
       "      <td>ITê³¼í•™</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=105&amp;sid2=230&amp;oid=001&amp;aid=0010811432</td>\n",
       "      <td>2019.05.08. ì˜¤ì „ 11:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ynat-v1_train_18498</td>\n",
       "      <td>æ—¥ì˜¤í‚¤ë‚˜ì™€ ì£¼ë¯¼ë“¤ ç¾ê¸°ì§€ ì¡°ì„± ë°˜ëŒ€ ìœ¡Â·í•´ìƒ ì‹œìœ„</td>\n",
       "      <td>ì„¸ê³„</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=104&amp;sid2=231&amp;oid=001&amp;aid=0010716506</td>\n",
       "      <td>2019.03.25. ì˜¤í›„ 4:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ynat-v1_train_36327</td>\n",
       "      <td>æ—¥ ì••ì¶•ê³µê¸°ë¡œ ê±´ë¬¼ ë„ì›Œ ì§€ì§„ ì¶©ê²© ì°¨ë‹¨ì¥ì¹˜ ê°œë°œ</td>\n",
       "      <td>ì„¸ê³„</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=105&amp;sid2=228&amp;oid=001&amp;aid=0009522139</td>\n",
       "      <td>2017.09.05. ì˜¤ì „ 7:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ynat-v1_train_17934</td>\n",
       "      <td>ë´„ì˜ ì „ë ¹ 2016 ì²­ì‚°ë„ ìŠ¬ë¡œ ê±·ê¸° ì¶•ì œ</td>\n",
       "      <td>ìƒí™œë¬¸í™”</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=103&amp;sid2=237&amp;oid=001&amp;aid=0008268281</td>\n",
       "      <td>2016.03.21. ì˜¤ì „ 10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ynat-v1_train_33727</td>\n",
       "      <td>êµ­ë¯¼ 3ëª… ì¤‘ 2ëª…ì€ ê²Œì´ë¨¸â€¦í•˜ë£¨ 90ë¶„ì´ìƒ ëª¨ë°”ì¼ë¡œ ì¦ê²¨</td>\n",
       "      <td>ITê³¼í•™</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=105&amp;sid2=226&amp;oid=001&amp;aid=0010330240</td>\n",
       "      <td>2018.09.09. ì˜¤ì „ 8:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ynat-v1_train_31752</td>\n",
       "      <td>ç¾ ì´ì‚°ê°€ì¡±ë‹¨ì²´ íŠ¸ëŸ¼í”„ ë°©ë¶í•´ ì´ì‚°ìƒë´‰ ë…¼ì˜ ì´‰êµ¬ì„œí•œ</td>\n",
       "      <td>ì„¸ê³„</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=100&amp;sid2=268&amp;oid=001&amp;aid=0008923125</td>\n",
       "      <td>2016.12.29. ì˜¤í›„ 2:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ynat-v1_train_00293</td>\n",
       "      <td>ë² ë„¤ìˆ˜ì—˜ë¼ ì „ êµ°ì •ë³´ê¸°ê´€ì¥ ì¹´ë¥´ë°”í•  ìŠ¤í˜ì¸ì„œ íˆ¬ì˜¥</td>\n",
       "      <td>ì„¸ê³„</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=104&amp;sid2=233&amp;oid=001&amp;aid=0010761042</td>\n",
       "      <td>2019.04.14. ì˜¤ì „ 2:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ynat-v1_train_42467</td>\n",
       "      <td>ë‚¨ë™ê³µë‹¨ í™”ì¬ í¬ìƒì 9ëª… í•©ë™ë¶„í–¥ì†Œ ê¸¸ë³‘ì›ì— ë§ˆë ¨</td>\n",
       "      <td>ì‚¬íšŒ</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=102&amp;sid2=257&amp;oid=001&amp;aid=0010287262</td>\n",
       "      <td>2018.08.22. ì˜¤í›„ 1:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ynat-v1_train_43684</td>\n",
       "      <td>KT 5G ìŠ¤ì¹´ì´ì‹­ ITU ì–´ì›Œì¦ˆ ê¸€ë¡œë²Œ ì‚°ì—…ìƒ ìˆ˜ìƒ</td>\n",
       "      <td>ITê³¼í•™</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=105&amp;sid2=227&amp;oid=001&amp;aid=0011080667</td>\n",
       "      <td>2019.09.15. ì˜¤ì „ 10:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ynat-v1_train_17277</td>\n",
       "      <td>ì¼€ì¸ ë„¤ì´ë§ˆë¥´ ì´ì–´ ëª¸ê°’ 2ìœ„ ë˜ë‚˜â€¦ë ˆì•Œ 2ì–µ ìœ ë¡œ ì¤€ë¹„</td>\n",
       "      <td>ìŠ¤í¬ì¸ </td>\n",
       "      <td>https://sports.news.naver.com/news.nhn?oid=001&amp;aid=0009588447</td>\n",
       "      <td>2017.10.04 21:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ynat-v1_train_33837</td>\n",
       "      <td>ì˜¤ì‚¬ì¹´ì„œ ì°¸ì˜ì› ì„ ê±° ìœ ì„¸í•˜ëŠ” ì•„ë²  ì´ë¦¬</td>\n",
       "      <td>ì„¸ê³„</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=104&amp;sid2=231&amp;oid=001&amp;aid=0010936941</td>\n",
       "      <td>2019.07.07. ì˜¤ì „ 11:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ynat-v1_train_36117</td>\n",
       "      <td>ì•„ë¹„ìŠ¤íƒ€ 50ì–µì› 3ìë°°ì • ìœ ìƒì¦ì ê²°ì •</td>\n",
       "      <td>ê²½ì œ</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=101&amp;sid2=259&amp;oid=001&amp;aid=0008884826</td>\n",
       "      <td>2016.12.12. ì˜¤í›„ 4:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ynat-v1_train_13098</td>\n",
       "      <td>4ë²ˆíƒ€ì ê¹€í•˜ì„± ì„ ì œ íˆ¬ëŸ°í¬â€¦ì¥ì›ì¤€ 4ê²½ê¸° ì—°ì† í”¼í™ˆëŸ°</td>\n",
       "      <td>ìŠ¤í¬ì¸ </td>\n",
       "      <td>https://sports.news.naver.com/news.nhn?oid=001&amp;aid=0010024377</td>\n",
       "      <td>2018.04.14 17:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ynat-v1_train_34322</td>\n",
       "      <td>ì˜¬ë¦¼í”½ ê°•ë¦‰ ë¯¸ë””ì–´ì´Œ ë¼ìš´ì§€ ì „í†µë¬¸í™” ì²´í—˜</td>\n",
       "      <td>ìƒí™œë¬¸í™”</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=103&amp;sid2=242&amp;oid=001&amp;aid=0009900412</td>\n",
       "      <td>2018.02.18. ì˜¤í›„ 1:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ynat-v1_train_38510</td>\n",
       "      <td>ë¬¸ëŒ€í†µë ¹ êµ­ì •ì§€ì§€ë„ 45%ë¡œ 1%p ìƒìŠ¹â€¦ë¶€ì •í‰ê°€ 44%í•œêµ­ê°¤ëŸ½</td>\n",
       "      <td>ì •ì¹˜</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=100&amp;sid2=265&amp;oid=001&amp;aid=0010711162</td>\n",
       "      <td>2019.03.22. ì˜¤ì „ 10:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ynat-v1_train_24097</td>\n",
       "      <td>ë”ìš¸ ë• ì„œì ìœ¼ë¡œâ€¦êµë³´ë¬¸ê³  ì—¬ë¦„ë‚˜ê¸° ì´ë²¤íŠ¸</td>\n",
       "      <td>ìƒí™œë¬¸í™”</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D&amp;mid=shm&amp;sid1=103&amp;sid2=243&amp;oid=001&amp;aid=0010989860</td>\n",
       "      <td>2019.07.29. ì˜¤ì „ 11:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ê° columnì˜ êµ¬ì„±ì„ ì„ì˜ì˜ ìƒ˜í”Œì„ ì¶”ì¶œí•˜ì—¬ ì‚´í´ë³´ì.\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(dataset[\"train\"], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae9b18-97e5-48b8-b946-677486d0edd3",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "### Tokenizer load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ae6487-aa2a-4cb4-8c32-771b0233e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d02f234f-27db-4e94-af4f-053a6f6c04e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8506a669-a072-48f0-b3d8-47b401262114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['title'], truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7b236c4-a1e9-4ca0-82a0-16f824ef1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6e5366a-6cb3-405e-9414-81949b6b9c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'ynat-v1_train_00000',\n",
       " 'title': 'ìœ íŠœë¸Œ ë‚´ë‹¬ 2ì¼ê¹Œì§€ í¬ë¦¬ì—ì´í„° ì§€ì› ê³µê°„ ìš´ì˜',\n",
       " 'label': 3,\n",
       " 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
       " 'date': '2016.06.30. ì˜¤ì „ 10:36',\n",
       " 'input_ids': [0,\n",
       "  10637,\n",
       "  8474,\n",
       "  22,\n",
       "  2210,\n",
       "  2299,\n",
       "  2118,\n",
       "  28940,\n",
       "  3691,\n",
       "  4101,\n",
       "  3792,\n",
       "  2],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc0a06e-af4e-45e5-9c90-606a7e0e5154",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3cac088-2356-405d-9a3a-65456d47e4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = 7\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abd84dd-b554-4175-8f71-2d05e5c68f21",
   "metadata": {},
   "source": [
    "# Parameter Setting\n",
    " HuggingFace ì—ì„œëŠ” __Trainer__ ê°ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.   \n",
    " ì´ë•Œ, Trainer ê°ì²´ëŠ” ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ ì„¤ì •í•´ì•¼ í•˜ëŠ” ê°’ì´ ë“¤ì–´ìˆëŠ” í´ë˜ìŠ¤ì¸ __TrainingArgument__ ë¥¼ ì…ë ¥ë°›ì•„ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dd84630-a6bb-4e44-abfc-41d4ef24e88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kyeul\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (23.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\kyeul\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from accelerate) (2.1.0+cu121)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from accelerate) (0.17.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.9.2)\n",
      "Requirement already satisfied: requests in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kyeul\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (4.35.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kyeul\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kyeul\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kyeul\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U accelerate\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c0f7098-e9cd-4762-a691-1984ed192499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.35.0', '0.24.1')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "transformers.__version__, accelerate.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34b1ce45-f896-4d06-ba4e-05ad76315491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyeul\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\training_args.py:1716: FutureWarning: `--push_to_hub_model_id` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_model_id` instead and pass the full repo name to this argument (in this case kyeul611/roberta-large-finetuned-ynat).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "output_dir = os.path.join(\"test-klue\", \"ynat\") # task ë³„ë¡œ ë°”ê¿”ì£¼ë©° ì‚¬ìš©\n",
    "logging_dir = os.path.join(output_dir, 'logs')\n",
    "args = TrainingArguments(\n",
    "    # checkpoint, ëª¨ë¸ì˜ checkpointê°€ ì €ì¥ë˜ëŠ” ìœ„ì¹˜\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True, # ë®ì–´ì“°ê¸°ê² ì§€?\n",
    "\n",
    "    # Model save and load\n",
    "    save_strategy=\"epoch\", # or \"steps\"\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps=500,\n",
    "\n",
    "    # Dataset, epochì™€ batch_size\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "\n",
    "    # Optimizer\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    fp16=True,\n",
    "\n",
    "    # Regualarization\n",
    "    # max_grad_norm = 1.0,\n",
    "    # label_smoothing_factor=0.1,\n",
    "\n",
    "    # Evaluation\n",
    "    metric_for_best_model='eval_f1', # task ë³„ í‰ê°€ì§€í‘œ\n",
    "    evaluation_strategy=\"epoch\",\n",
    "\n",
    "    # HuggingFace Hub upload\n",
    "    push_to_hub=True,\n",
    "    push_to_hub_model_id=f\"{model_name}-finetuned-{task}\",\n",
    "\n",
    "    # Logging, log ê¸°ë¡ì„ ì‚´í´ë³¼ ìœ„ì¹˜, wandbë¥¼ ì‚¬ìš©í•˜ì\n",
    "    logging_dir=logging_dir,\n",
    "    report_to='wandb',\n",
    "\n",
    "    # Seed\n",
    "    seed=1,\n",
    ")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "446f9210-240f-4b26-9dad-3bae4f1b7399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyeul\\AppData\\Local\\Temp\\ipykernel_15008\\3881143826.py:4: FutureWarning: list_metrics is deprecated and will be removed in the next major version of datasets. Use 'evaluate.list_evaluation_modules' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metrics_list = list_metrics()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy, bertscore, bleu, bleurt, brier_score, cer, character, charcut_mt, chrf, code_eval, comet, competition_math, coval, cuad, exact_match, f1, frugalscore, glue, google_bleu, indic_glue, mae, mahalanobis, mape, mase, matthews_correlation, mauve, mean_iou, meteor, mse, nist_mt, pearsonr, perplexity, poseval, precision, r_squared, recall, rl_reliability, roc_auc, rouge, sacrebleu, sari, seqeval, smape, spearmanr, squad, squad_v2, super_glue, ter, trec_eval, wer, wiki_split, xnli, xtreme_s, AlhitawiMohammed22/CER_Hu-Evaluation-Metrics, BucketHeadP65/confusion_matrix, BucketHeadP65/roc_curve, DarrenChensformer/eval_keyphrase, DarrenChensformer/relation_extraction, Drunper/metrica_tesi, Felipehonorato/eer, GMFTBY/dailydialog_evaluate, GMFTBY/dailydialogevaluate, He-Xingwei/sari_metric, Ikala-allen/relation_extraction, JP-SystemsX/nDCG, Josh98/nl2bash_m, KevinSpaghetti/accuracyk, Muennighoff/code_eval_octopack, NCSOFT/harim_plus, Natooz/ece, NikitaMartynov/spell-check-metric, NimaBoscarino/weat, Ochiroo/rouge_mn, Pipatpong/perplexity, SpfIo/wer_checker, Splend1dchan/cosine_similarity, Vallp/ter, Vertaix/vendiscore, Viona/fuzzy_reordering, Viona/infolm, Viona/kendall_tau, Vipitis/shadermatch, Vlasta/pr_auc, Yeshwant123/mcc, abdusah/aradiawer, abidlabs/mean_iou, abidlabs/mean_iou2, amitness/perplexity, andstor/code_perplexity, angelina-wang/directional_bias_amplification, anz2/iliauniiccocrevaluation, aryopg/roc_auc_skip_uniform_labels, bdsaglam/jer, brian920128/doc_retrieve_metrics, bstrai/classification_report, bugbounty1806/accuracy, cakiki/ndcg, carletoncognitivescience/peak_signal_to_noise_ratio, chanelcolgate/average_precision, ckb/unigram, codeparrot/apps_metric, cpllab/syntaxgym, daiyizheng/valid, danieldux/hierarchical_softmax_loss, dvitel/codebleu, ecody726/bertscore, erntkn/dice_coefficient, fschlatt/ner_eval, gabeorlanski/bc_eval, giulio98/code_eval_outputs, giulio98/codebleu, gnail/cosine_similarity, gorkaartola/metric_for_tp_fp_samples, guydav/restrictedpython_code_eval, hack/test_metric, harshhpareek/bertscore, hpi-dhc/FairEval, hynky/sklearn_proxy, hyperml/balanced_accuracy, idsedykh/codebleu, idsedykh/codebleu2, idsedykh/megaglue, idsedykh/metric, illorca/FairEval, ingyu/klue_mrc, jjkim0807/code_eval, jordyvl/ece, jpxkqx/peak_signal_to_noise_ratio, jpxkqx/signal_to_reconstruction_error, jzm-mailchimp/joshs_second_test_metric, k4black/codebleu, kaggle/ai4code, kaggle/amex, kashif/mape, kedudzic/charmatch, kyokote/my_metric2, langdonholmes/cohen_weighted_kappa, leslyarun/fbeta_score, lhy/hamming_loss, lhy/ranking_loss, loubnabnl/apps_metric2, lvwerra/accuracy_score, lvwerra/bary_score, lvwerra/test, manueldeprada/beer, mfumanelli/geometric_mean, mgfrantz/roc_auc_macro, mtc/fragments, nlpln/tst, ola13/precision_at_k, omidf/squad_precision_recall, posicube/mean_reciprocal_rank, repllabs/mean_average_precision, repllabs/mean_reciprocal_rank, ronaldahmed/nwentfaithfulness, sakusakumura/bertscore, shalakasatheesh/squad, shirayukikun/sescore, shunzh/apps_metric, sma2023/wil, sportlosos/sescore, tialaeMceryu/unigram, transZ/sbert_cosine, transZ/test_parascore, transformersegmentation/segmentation_scores, unitxt/metric, unnati/kendall_tau_distance, vichyt/metric-codebleu, weiqis/pajm, xu1998hz/sescore, xu1998hz/sescore_english_coco, xu1998hz/sescore_english_mt, xu1998hz/sescore_english_webnlg, xu1998hz/sescore_german_mt, ybelkada/cocoevaluate, yonting/average_precision_score, yulong-me/yl_metric, yuyijiong/quad_match_score, yzha/ctc_eval, zbeloki/m2\n"
     ]
    }
   ],
   "source": [
    "# Set metrics\n",
    "# metric list í™•ì¸\n",
    "from datasets import list_metrics, load_metric\n",
    "metrics_list = list_metrics()\n",
    "len(metrics_list)\n",
    "print(', '.join(metric for metric in metrics_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da89efd9-1c32-428b-84c9-4d75fbc61da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyeul\\AppData\\Local\\Temp\\ipykernel_15008\\2797725921.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric_macrof1 = load_metric('f1')\n"
     ]
    }
   ],
   "source": [
    "# we use metric for f1\n",
    "metric_macrof1 = load_metric('f1')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = eval_pred.predictions.argmax(-1)\n",
    "    labels = eval_pred.label_ids\n",
    "    return metric_macrof1.compute(predictions=predictions,\n",
    "                                    references=labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8e3b63e-76b6-45a2-a46a-1c8861ebc2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba8b13-2923-431c-b22f-728222425dbf",
   "metadata": {},
   "source": [
    "# Training!\n",
    "## First, we set the wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6255299d-8f7a-41c9-97ff-22fa3deab940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgyul611\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d551966-6519-4f4e-9777-a65941641662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0lg6r2il\n"
     ]
    }
   ],
   "source": [
    "id = wandb.util.generate_id()\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23e67224-3ea9-4c72-bd3c-ffed4200f5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\kyeul\\Desktop\\code\\practice_language\\python\\Deeplearning\\huggingface\\tutorials\\Fine-tuning_Baseline\\wandb\\run-20231111_151635-0lg6r2il</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gyul611/Klue-ynat/runs/0lg6r2il' target=\"_blank\">ynat</a></strong> to <a href='https://wandb.ai/gyul611/Klue-ynat' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gyul611/Klue-ynat' target=\"_blank\">https://wandb.ai/gyul611/Klue-ynat</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gyul611/Klue-ynat/runs/0lg6r2il' target=\"_blank\">https://wandb.ai/gyul611/Klue-ynat/runs/0lg6r2il</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/gyul611/Klue-ynat/runs/0lg6r2il?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x215a24aedd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Klue-ynat\",\n",
    "           entity='gyul611',\n",
    "           id=id,\n",
    "           name='ynat',\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244d37c-5173-4b44-9905-5355b2a2894a",
   "metadata": {},
   "source": [
    "## Let's fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d7101bc-70f6-44dd-a6e8-812a5db2690e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.24 GiB is allocated by PyTorch, and 88.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\trainer.py:1546\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[0;32m   1545\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[1;32m-> 1546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1553\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\transformers\\trainer.py:1910\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1904\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[0;32m   1905\u001b[0m             model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[0;32m   1906\u001b[0m             args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[0;32m   1907\u001b[0m         )\n\u001b[0;32m   1909\u001b[0m \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n\u001b[1;32m-> 1910\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1911\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run:\n\u001b[0;32m   1913\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\accelerate\\optimizer.py:132\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_patched_step_method\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called:\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;66;03m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:416\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    414\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 416\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:315\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m--> 315\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\accelerate\\optimizer.py:185\u001b[0m, in \u001b[0;36mpatch_optimizer_step.<locals>.patched_step\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpatched_step\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    184\u001b[0m     accelerated_optimizer\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\optim\\adamw.py:184\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    171\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    174\u001b[0m         group,\n\u001b[0;32m    175\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m         state_steps,\n\u001b[0;32m    182\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\optim\\adamw.py:335\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 335\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlp\\Lib\\site-packages\\torch\\optim\\adamw.py:599\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    597\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 599\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[0;32m    602\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.24 GiB is allocated by PyTorch, and 88.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f2667-035b-41c7-bd45-3d4bec8e433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµì´ ëë‚˜ë©´ wandbë„ ì¢…ë£Œ\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e1afe-e4f9-4860-bddc-9d0670a1114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"Klue-ynat\",\n",
    "           entity='kyeul611',\n",
    "           id=id,\n",
    "           name='ynat',\n",
    "          )\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff14e4-b1df-4ed1-9916-ddc4a4943c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4469d-2f9c-41d1-babd-68a02282fe74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
