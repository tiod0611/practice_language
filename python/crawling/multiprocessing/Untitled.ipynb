{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c2cc62-f18c-4024-9f78-d4e3bc4e85e7",
   "metadata": {},
   "source": [
    "This code is sourced from [this post](https://medium.com/@kunal.rustagi/boost-your-web-crawler-using-multiple-processes-in-python-3cc3ff519226).\n",
    "\n",
    "이 노트북은 medium에서 Kunal Rustagi의 \"Boost your web crwaler using multiple processes in Python\"이란 포스트 내용을 한글로 번역하여 담고 있습니다. 자세한 내용은 원문을 참고하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deacddf1-b8c1-4e82-a8b6-c25a65d02447",
   "metadata": {},
   "source": [
    "# Boost your web crawler using multiple processes in Python\n",
    "# 멀티 프로세스를 사용해 파이썬 웹 크롤러 속도를 향상시키기.\n",
    "\n",
    "이 글에서 우리는 멀티프로세싱을 사용해 크롤러가 더 빠르게 일하도록 만들겁니다.  \n",
    "\n",
    "\n",
    "지금까지 우리는 웹페이지에서 데이터를 긁어 오는 것으로 데이터셋을 구축하고, 또 여러 웹페이지를 수집하려고 자체 웹 크롤러를 만들었습니다. 이 두가지 글의 링크는 [이곳](https://medium.com/@kunal.rustagi/web-scraping-for-beginners-using-python-73f0966d895f)과 [이곳](https://medium.com/@kunal.rustagi/building-a-web-crawler-to-scrape-data-from-multiple-pages-12799103b768)에서 찾을 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "## Timing, timing, timing!!\n",
    "## 타이밍, 타이밍, 타이밍!!\n",
    "### 멀티 프로세싱이 없이..\n",
    "\n",
    "만약 이전에 작성한 웹 스크래핑 부분을 실행하면, 우리는 다음 방법으로 시간 주기를 계산할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad0d3ea0-9804-41b6-b7b7-b5f5e68c0308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 55.3401141166687 seconds\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "\n",
    "pages = np.arange(1, 21, 1)\n",
    "\n",
    "book_title = [] # 책 제목\n",
    "star_rating = [] # 별점\n",
    "product_price = [] # 가격\n",
    "\n",
    "start = time.time()\n",
    "for page in pages:\n",
    "    time.sleep(random.randint(1, 3))\n",
    "    \n",
    "    url = 'http://books.toscrape.com/catalogue/page-' + str(page) + '.html'\n",
    "    results = requests.get(url)\n",
    "    soup = BeautifulSoup(results.text, 'html.parser')\n",
    "    \n",
    "    book_div = soup.find_all('li', class_='col-xs-6 col-sm-4 col-md-3 col-lg-3')\n",
    "    \n",
    "    for container in book_div:\n",
    "        title = container.article.h3.a['title']\n",
    "        book_title.append(title)\n",
    "        \n",
    "        price = container.article.find('div', class_ = 'product_price').p.text\n",
    "        product_price.append(price)\n",
    "        \n",
    "        rating = container.article.p['class'][-1]\n",
    "        star_rating.append(rating)\n",
    "\n",
    "end = time.time()\n",
    "print(\"It took {} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71394d5-30f3-4bd3-8337-7c7a6a5478d5",
   "metadata": {},
   "source": [
    "이렇게 걸린 시간은 몹시 큽니다. 하지만 파이썬은 멀티 프로세싱을 제공합니다. 이 기능은 크롤링 시간을 상당히 줄일 수 있게 합니다.\n",
    "\n",
    "## Multiprocessing\n",
    "## 멀티프로세싱\n",
    "\n",
    "파이썬의 [멀티프로세싱](https://docs.python.org/3.6/library/multiprocessing.html)은 스레딩 모듈과 유사한 API를 사용해 프로세스 생성을 지원하는 패키지입니다. 멀티프로세싱 패키지는 로컬과 원격의 동시성을 지원하고, 스레드 대신 subprocesses를 사용해 _Global Interpreter Lock_(전역 인터프리터 락)을 효과적으로 피할 수 있습니다. 이런 이유로 멀티프로세싱은 프로그래머가 컴퓨터의 멀티 프로세서를 완전히 효과적으로 사용할 수 있게 해줍니다. 멀티 프로세싱은 Unix와 Windows에서 모두 작동합니다.\n",
    "\n",
    "쉽게 말해, 멀티 프로세싱은 컴퓨터의 프로세스를 여러개의 작은 프로세스로 쪼개고 이 프로세스들을 병렬적으로 실행하는 것입니다. (그래서 하드웨어에 크게 의존합니다.) 일반적으로 멀티프로세싱은 소프트웨어보다는 하드웨어적인 의미를 이야기합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 이전 코드를 수정해볼까요.\n",
    "### 라이브러리, 스트럭처 그리고 페이지 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2668e7a5-1571-4f1c-9cdf-94a6be1b3083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from multiprocessing import Pool # 멀티프로세싱\n",
    "\n",
    "url_list = []\n",
    "\n",
    "pages = np.arange(1, 51, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0411fa-79e9-4162-aeca-7a34cda9abdc",
   "metadata": {},
   "source": [
    "이전 코드와 비교하면, 새로운 몇 줄이 추가되었습니다.\n",
    "1. `multiprocessing` 라이브러리에서 `Pool` class를 import했습니다.\n",
    "2. URL들을 저장할 `url_list` 리스트 변수를 만들었습니다.\n",
    "\n",
    "## Creating new functions\n",
    "## 새로운 함수 생성\n",
    "이전 코드에서 몇몇 부분을 함수 안으로 옮길겁니다. 이 함수는 코드 반복을 피하게 하고 가독성을 올려주면서 복잡한 문제를 좀 더 단순한 문제로 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27aac3d-bb71-468e-bdcf-9da3e39df380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from multiprocessing import Pool # 멀티프로세싱\n",
    "\n",
    "url_list = []\n",
    "\n",
    "pages = np.arange(1, 51, 1)\n",
    "\n",
    "## 추가되는 부분\n",
    "def generate_urls(pages):\n",
    "    for page in pages:\n",
    "        url = 'http://books.toscrape.com/catalogue/page-'+str(page)+'.html'\n",
    "        url_list.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f43210-0c19-497b-8a3b-c4865332dbd4",
   "metadata": {},
   "source": [
    "`generate_url()` 함수는 모든 urls을 생성하고 `url_list`에 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fdcd5da-77f6-4fb7-89ac-b3f97be89a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from multiprocessing import Pool # 멀티프로세싱\n",
    "\n",
    "url_list = []\n",
    "\n",
    "pages = np.arange(1, 31, 1)\n",
    "\n",
    "def generate_urls():\n",
    "    for page in pages:\n",
    "        url = 'http://books.toscrape.com/catalogue/page-'+str(page)+'.html'\n",
    "        url_list.append(url)\n",
    "        \n",
    "## 추가되는 부분\n",
    "def scape_url(url):\n",
    "    book_title = []\n",
    "    star_rating = []\n",
    "    product_price = []\n",
    "    \n",
    "    time.sleep(random.randint(1, 2))\n",
    "    results = requests.get(url)\n",
    "    soup = BeautifulSoup(results.text, 'html.parser')\n",
    "    \n",
    "    book_div = soup.find_all(\"li\", class_ = \"col-xs-6 col-sm-4 col-md-3 col-lg-3\")\n",
    "    \n",
    "    for container in book_div:\n",
    "        title = container.article.h3.a['title']\n",
    "        book_title.append(title)\n",
    "        \n",
    "        price = container.article.find('div', class_='product_price').p.text\n",
    "        product_price.append(price)\n",
    "        \n",
    "        rating = container.article.p['class'][-1]\n",
    "        star_rating.append(rating)\n",
    "        \n",
    "    return (book_title, product_price, star_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15898fbb-cce9-42fe-86e8-71749d951138",
   "metadata": {},
   "source": [
    "대부분의 웹 스크래핑 코드는 `scape_url()`로 옮겼습니다. 마지막에는 `book_title`, `product_price` 그리고 `star_rating` 리스트를 튜플로 감싸 반환합니다.\n",
    "\n",
    "## Filling `url_list`\n",
    "## `url_list` 채우기\n",
    "\n",
    "이제 `generate_url()`함수를 호출하고 처음 10개의 urls을 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d143bc6-ce73-4e9a-8ccd-b2d6f2471dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Pool() 부분에서 제대로 작동하지 않아서 원인 파악하기.\n",
    "url_list = generate_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e226f48-02f6-42d7-a593-e1969c06339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553aec0c-fe5b-4508-9657-18f7ef453526",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12016\\2748905564.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0murl_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscape_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for i in url_list[:4]:\n",
    "    for (title, price, rating) in scape_url(i):\n",
    "        print(title, price, rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "330b0cc2-ead9-48fc-95d4-d0b8bf834b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://books.toscrape.com/catalogue/page-1.html\n",
      "http://books.toscrape.com/catalogue/page-2.html\n",
      "http://books.toscrape.com/catalogue/page-3.html\n",
      "http://books.toscrape.com/catalogue/page-4.html\n",
      "http://books.toscrape.com/catalogue/page-5.html\n",
      "http://books.toscrape.com/catalogue/page-6.html\n",
      "http://books.toscrape.com/catalogue/page-7.html\n",
      "http://books.toscrape.com/catalogue/page-8.html\n",
      "http://books.toscrape.com/catalogue/page-9.html\n",
      "http://books.toscrape.com/catalogue/page-10.html\n"
     ]
    }
   ],
   "source": [
    "generate_urls()\n",
    "\n",
    "for i in url_list[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d315385f-dffa-4ff2-982c-6a7d9a0b835d",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd1344d8-956b-45d5-b55b-636821682fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b60f7-8cdc-4d68-bcdf-30b8c7cb5914",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pool(10) # 10개의 워커를 생성함\n",
    "book_list = p.map(scape_url, url_list) # 함수와 인자를 매핑하여 실행까지..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42388d42-6f73-4ba2-8693-575544a130ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703927fc-9ad7-4037-8aa5-ecdc0c67fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0c68e-8fbf-4146-a5b9-42299803c464",
   "metadata": {},
   "source": [
    "+ `Pool()` 클래스는 한번에 얼만큼의 워크 프로세스를 생성할지를 말합니다.여기서는 10을 인수로 전달했는데, 10개의 URL들이 동시에 처리하게 됩니다.\n",
    "+ `map()`은 첫번째 인자로 `scrape_url`을 그리고 두번째 인자로 `url_list`를 받습니다. 50개의 페이지들은 한번에 10개씩 처리됩니다. 그러면 5번의 반복이 수행될 겁니다.\n",
    "+ `terminate()`는 프로세스를 제거합니다.\n",
    "+ `join()`은 워크 프로세스가 빠져나올때까지 기다리기 위해 사용됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "## Check time taken\n",
    "## 걸린 시간 확인하기\n",
    "`start`와 `end`를 코드에 추가해서 시간을 계산해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b91523-5ad0-401e-afc0-cb602a712d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
