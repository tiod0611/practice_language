{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4a92c4f-1bfc-4ad3-b744-f68c10d20aad",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f583b5fd-0019-48ee-b5cc-637524fe3267",
   "metadata": {},
   "source": [
    "## Load Dataset from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c546e3e-ee7b-4c76-8a1e-180b83862a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00f67135-e220-401b-9d7f-1b99188a9348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80f61243-b8aa-467a-8efe-aba94efa952d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils._bunch.Bunch"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee6e9d53-b7a5-460b-b01c-668bcfd75eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  class  \n",
       "564                0.2216          0.2060                  0.07115      0  \n",
       "565                0.1628          0.2572                  0.06637      0  \n",
       "566                0.1418          0.2218                  0.07820      0  \n",
       "567                0.2650          0.4087                  0.12400      0  \n",
       "568                0.0000          0.2871                  0.07039      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "df['class'] = cancer.target\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ca2b6ff-3d3a-4c7a-9711-e01d0d0c6532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 mean radius\n",
      "1 mean texture\n",
      "2 mean perimeter\n",
      "3 mean area\n",
      "4 mean smoothness\n",
      "5 mean compactness\n",
      "6 mean concavity\n",
      "7 mean concave points\n",
      "8 mean symmetry\n",
      "9 mean fractal dimension\n",
      "10 radius error\n",
      "11 texture error\n",
      "12 perimeter error\n",
      "13 area error\n",
      "14 smoothness error\n",
      "15 compactness error\n",
      "16 concavity error\n",
      "17 concave points error\n",
      "18 symmetry error\n",
      "19 fractal dimension error\n",
      "20 worst radius\n",
      "21 worst texture\n",
      "22 worst perimeter\n",
      "23 worst area\n",
      "24 worst smoothness\n",
      "25 worst compactness\n",
      "26 worst concavity\n",
      "27 worst concave points\n",
      "28 worst symmetry\n",
      "29 worst fractal dimension\n",
      "30 class\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate(list(df.columns)):\n",
    "    print(i, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb6460-f9b7-4578-84c5-7c1bf8b0fd5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert to PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65460c06-4382-406c-912f-f4514f9c7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab585bcd-fa70-46ba-ad4f-b7e943e52ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([569, 31])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.from_numpy(df.values).float() #torch.from_numpy(data) 까지만 하면 data type이 double이 된다. float()로 변환을 하면 메모리 효율을 올릴 수 있겠죠?\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6df7fa58-6802-4411-872d-2624e60cfcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([569, 10]) torch.Size([569, 1])\n"
     ]
    }
   ],
   "source": [
    "x = data[:, :10]\n",
    "y = data[:, -1:]\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa0c0977-c2f5-4e21-8f71-778185b6766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Valid / Test\n",
    "ratio = [.6, .2, .2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bf460b7-877f-4e06-8807-e20708c8ddb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341 113 115\n"
     ]
    }
   ],
   "source": [
    "train_cnt = int(data.size(0) * ratio[0])\n",
    "valid_cnt = int(data.size(0) * ratio[1])\n",
    "test_cnt = int(data.size(0)) - train_cnt - valid_cnt\n",
    "cnts = [train_cnt, valid_cnt, test_cnt]\n",
    "\n",
    "print(train_cnt, valid_cnt, test_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09ff89a5-6d68-4571-b347-bf3f2b9a636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([341, 10]) torch.Size([341, 1])\n",
      "torch.Size([113, 10]) torch.Size([113, 1])\n",
      "torch.Size([115, 10]) torch.Size([115, 1])\n"
     ]
    }
   ],
   "source": [
    "indices =  torch.randperm(data.size(0))\n",
    "\n",
    "x = torch.index_select(x, dim=0, index=indices)\n",
    "y = torch.index_select(y, dim=0, index=indices)\n",
    "\n",
    "x = x.split(cnts, dim=0)\n",
    "y = y.split(cnts, dim=0)\n",
    "\n",
    "for x_i, y_i in zip(x, y):\n",
    "    print(x_i.size(), y_i.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd15768-9371-4b0d-9814-b1bdc4d84ed4",
   "metadata": {},
   "source": [
    "### You don't need to shuffle them, if you have fixed train, valid, test set.\n",
    "### e.g) MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f6c01-f16a-4d55-9b5b-8b9f176721e9",
   "metadata": {},
   "source": [
    "# Set Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c7d8480-3803-467c-9a33-985f8a48939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "batch_size = 128\n",
    "print_interval = 500\n",
    "early_stop = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f12dc1-3710-4f8a-9185-73202b41ac05",
   "metadata": {},
   "source": [
    "## Get DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6de5b11f-fa51-463a-b04f-5182fc258945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd507f6b-049e-4e7b-a593-1dbfb69b1332",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    # 3개의 함수만 over ride 해도 충분히 쓸 수 있다.\n",
    "    \n",
    "    def __init__(self, data, labels):\n",
    "        # 데이터 읽어오기\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 데이터 크기 반환\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 전처리 및 minibatch를 위한 sample을 반환함\n",
    "        # 실제 미니배치가 만들어지는 부분\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d37001e0-6922-4491-8f93-644d8914b2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 341 / Valid 113 / Test 115 sample.\n"
     ]
    }
   ],
   "source": [
    "# DataLoader 실 사용\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = CustomDataset(x[0], y[0]),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True # Allow shuffling only for training set. # 여기는 꼭 True!\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    dataset = CustomDataset(x[1], y[1]),\n",
    "    batch_size = batch_size,\n",
    "    shuffle=False, # 여긴 왜 False? \n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset = CustomDataset(x[2], y[2]),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, # 검증을 진행할 때 같은 테스트를 볼 수 있다는 부분에서 False를 추천\n",
    ")\n",
    "\n",
    "print(\"Train %d / Valid %d / Test %d sample.\" %(\n",
    "    len(train_loader.dataset),\n",
    "    len(valid_loader.dataset),\n",
    "    len(test_loader.dataset),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb36f7-abfa-484b-ae59-e315d0bd0a80",
   "metadata": {},
   "source": [
    "# Build Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d51a7cec-1342-42a1-9b9e-f09901342601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=6, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.01)\n",
       "  (2): Linear(in_features=6, out_features=5, bias=True)\n",
       "  (3): LeakyReLU(negative_slope=0.01)\n",
       "  (4): Linear(in_features=5, out_features=4, bias=True)\n",
       "  (5): LeakyReLU(negative_slope=0.01)\n",
       "  (6): Linear(in_features=4, out_features=3, bias=True)\n",
       "  (7): LeakyReLU(negative_slope=0.01)\n",
       "  (8): Linear(in_features=3, out_features=1, bias=True)\n",
       "  (9): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(x[0].size(-1), 6),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(6, 5),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(5, 4),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(4, 3),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(3, y[0].size(-1)),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a45bbadd-2712-4017-a7e3-377391604f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a6dda6-2e2d-44eb-9194-b4d12b39190e",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92dac8de-5b73-44ee-85ee-099801e7be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "lowest_loss = np.inf\n",
    "best_model = None\n",
    "lowest_epoch = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cfd26ca-8f0f-4d79-9b05-663bda52823f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500: 0.25979559620221454 / 0.2952679693698883 \n",
      "1000: 0.22496080895264944 / 0.2588886618614197 \n",
      "1500: 0.16895892471075058 / 0.20854058861732483 \n",
      "2000: 0.1526962568362554 / 0.19410179555416107 \n",
      "2500: 0.1566708485285441 / 0.19755379855632782 \n",
      "3000: 0.15390855570634207 / 0.2000281810760498 \n",
      "3500: 0.1564700777331988 / 0.19717812538146973 \n",
      "4000: 0.15536450843016306 / 0.19199088215827942 \n",
      "4500: 0.1530818591515223 / 0.19060355424880981 \n",
      "5000: 0.14586085081100464 / 0.19241183996200562 \n",
      "5500: 0.1469742183883985 / 0.19196219742298126 \n",
      "6000: 0.14956123133500418 / 0.1992666870355606 \n",
      "6500: 0.1642073392868042 / 0.18533983826637268 \n",
      "7000: 0.1514857957760493 / 0.19731342792510986 \n",
      "7500: 0.15329907337824503 / 0.2029128223657608 \n",
      "8000: 0.14716694255669913 / 0.19569358229637146 \n",
      "8500: 0.14518898725509644 / 0.18503481149673462 \n",
      "9000: 0.1545379931728045 / 0.1821019947528839 \n",
      "9500: 0.14288120716810226 / 0.18191632628440857 \n",
      "10000: 0.14364910622437796 / 0.17703023552894592 \n"
     ]
    }
   ],
   "source": [
    "train_history, valid_history = [], []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    model.train() \n",
    "    \n",
    "    # We can remove following code block\n",
    "    # --------------------------------------------------\n",
    "    # indices = torch.randperm(x[0].size(0))\n",
    "    # x_ = torch.index_select(x[0], dim=0, index=indices)\n",
    "    # y_ = torch.index_select(y[0], dim=0, index=indices)\n",
    "    #\n",
    "    # x_ = x_.split(batch_size, dim=0)\n",
    "    # y_ = y_.split(batch_size, dim=0)\n",
    "    # --------------------------------------------------\n",
    "    \n",
    "    train_loss, valid_loss = 0, 0\n",
    "    y_hat = []\n",
    "    \n",
    "    for x_i, y_i in train_loader: # 과거에는 enumerate(x_, y_)를 했다면 이제는 train_loader만 하면 됨. 자동으로 mini batch를 만들어주기 때문이겠지.\n",
    "        y_hat_i = model(x_i)\n",
    "        loss = F.binary_cross_entropy(y_hat_i, y_i)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_loss += float(loss) # This is very important to prevent memory leak.\n",
    "        \n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        \n",
    "        for x_i, y_i in valid_loader:\n",
    "            y_hat_i = model(x_i)\n",
    "            loss = F.binary_cross_entropy(y_hat_i, y_i)\n",
    "            \n",
    "            valid_loss += float(loss)\n",
    "            \n",
    "            y_hat += [y_hat_i]\n",
    "            \n",
    "    valid_loss = valid_loss / len(valid_loader)\n",
    "    \n",
    "    train_history += [train_loss]\n",
    "    valid_history += [valid_loss]\n",
    "    \n",
    "    if (i+1) % print_interval==0:\n",
    "        print(f\"{i+1}: {train_loss} / {valid_loss} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb43177-3dd8-4b54-9f09-dd2f72a8afed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
