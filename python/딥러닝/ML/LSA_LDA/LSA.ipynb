{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6156cabe-acbd-440f-8d00-64a580a367c9",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition, SVD\n",
    "\n",
    "특이값 분해는 행렬을 3개의 행렬의 곱의 형태로 분해하는 것이다.  \n",
    "$ A = U\\Sigma V^T$  (직교, 대각, 직교)  \n",
    "직교행렬: n x n 행렬 A에 대하여 $ A \\times A^T = I$이면서 $A^T\\times A = I$인 행렬이다. 즉, $A^{-1} = A^T$를 만족한다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77617037-598f-4d52-9be4-323f39cc9c40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "561fd50c-2645-4d54-8e8b-51f718edae19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM의 크기(shape) : (4, 9)\n"
     ]
    }
   ],
   "source": [
    "A = np.array(\n",
    "    [\n",
    "        [0,0,0,1,0,1,1,0,0],\n",
    "        [0,0,0,1,1,0,1,0,0],\n",
    "        [0,1,1,0,2,0,0,0,0],\n",
    "        [1,0,0,0,0,0,0,1,1]\n",
    "    ]\n",
    ")\n",
    "print('DTM의 크기(shape) :', np.shape(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ec1be-efb2-4381-86a9-c9b9e230e7ac",
   "metadata": {},
   "source": [
    "Full SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4352a6f3-d563-4a54-811f-d245655beaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행렬 U :\n",
      "[[-0.24  0.75  0.   -0.62]\n",
      " [-0.51  0.44 -0.    0.74]\n",
      " [-0.83 -0.49 -0.   -0.27]\n",
      " [-0.   -0.    1.    0.  ]]\n",
      "행렬 U의 크기(shape) : (4, 4)\n"
     ]
    }
   ],
   "source": [
    "# 특이값 분해\n",
    "U, s, VT = np.linalg.svd(A, full_matrices=True)\n",
    "print('행렬 U :')\n",
    "print(U.round(2))\n",
    "print('행렬 U의 크기(shape) :',np.shape(U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d01da3-8741-4e00-a9bf-94d5942cc1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특이값 벡터 :\n",
      "[2.69 2.05 1.73 0.77]\n",
      "특이값 벡터의 크기(shape) : (4,)\n"
     ]
    }
   ],
   "source": [
    "print('특이값 벡터 :')\n",
    "print(s.round(2))\n",
    "print('특이값 벡터의 크기(shape) :',np.shape(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba0826-81d2-439d-b218-c4a8315745bf",
   "metadata": {},
   "source": [
    "Numpy의 linalg.svd()는 특이값 분해의 결과로 대각 행렬이 아니라 특이값의 리스트를 반환합니다.  \n",
    "그러므로 앞서 본 수식의 형식으로 보려면 이를 다시 대각 행렬로 바꾸어 주어야 합니다.  \n",
    "우선 특이값을 s에 저장하고 대각 행렬 크기의 행렬을 생성한 후에 그 행렬에 특이값을 삽입해도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3edd2aae-f8a6-45ae-bb83-1718afb25bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대각 행렬 S :\n",
      "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n",
      "대각 행렬의 크기(shape) :\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "# 대각 행렬의 크기인 4 x 9의 임의의 행렬 생성\n",
    "S = np.zeros((4, 9))\n",
    "\n",
    "# 특이값을 대각행렬에 삽입\n",
    "S[:4, :4] = np.diag(s) # diagonal 대각행렬을 만드는 함수\n",
    "\n",
    "print('대각 행렬 S :')\n",
    "print(S.round(2))\n",
    "\n",
    "print('대각 행렬의 크기(shape) :')\n",
    "print(np.shape(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd6c2edb-822b-409f-91fc-516942180677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.68731789, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 2.04508425, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 1.73205081, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.77197992]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(s) # 대각행렬의 값은 내림차순되어있"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f9b1746-5ce2-4f85-b25c-cf66ac425a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "직교행렬 VT :\n",
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
      " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
      " [ 0.   -0.35 -0.35  0.16  0.25 -0.8   0.16 -0.   -0.  ]\n",
      " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
      " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
      " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n",
      "직교 행렬 VT의 크기(shape) :\n",
      "(9, 9)\n"
     ]
    }
   ],
   "source": [
    "print('직교행렬 VT :')\n",
    "print(VT.round(2))\n",
    "\n",
    "print('직교 행렬 VT의 크기(shape) :')\n",
    "print(np.shape(VT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81501dc1-02ea-463f-99a0-cbec76d69827",
   "metadata": {},
   "source": [
    "분해된 행렬을 곱하면 원래 행렬이 나와야한다.  \n",
    "이 확인은 numpy의 allclose() 함수를 사용해서 알아보자. 두 행렬이 같으면 True를 리턴한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbea7c26-0338-4e8e-b05a-858f56baded1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(A, np.dot(np.dot(U,S), VT).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f97045-7448-4ef6-8479-c9722dcd1213",
   "metadata": {},
   "source": [
    "# 절단된 SVD\n",
    "절단된 SVD는 분해된 행렬의 t 크기만큼만 사용하게 되는데, 이 t값이 남길 정보량을 결정하는 하이퍼파라미터다.  \n",
    "t에는 중요한 정보가 남고 나머지는 상대적으로 의미가 없는 정보가 버려지게된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d4c1cb2-7fe7-4608-9d14-6ce16319cb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "절단된 대각 행렬 S: \n",
      "[[2.69 0.  ]\n",
      " [0.   2.05]]\n"
     ]
    }
   ],
   "source": [
    "S = S[:2, :2]\n",
    "\n",
    "print(\"절단된 대각 행렬 S: \")\n",
    "print(S.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9696e52-2f8c-4896-a5ef-587642cf85a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "절단된 행렬 U :\n",
      "[[-0.24  0.75]\n",
      " [-0.51  0.44]\n",
      " [-0.83 -0.49]\n",
      " [-0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "U = U[:, :2]\n",
    "print(\"절단된 행렬 U :\")\n",
    "print(U.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56887ba0-c5d7-413f-87b9-2da23719a7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "절단된 직교행렬 VT:\n",
      "[[-0.   -0.31 -0.31 -0.28 -0.8  -0.09 -0.28 -0.   -0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "VT = VT[:2, :]\n",
    "print(\"절단된 직교행렬 VT:\")\n",
    "print(VT.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "325fa70a-1dbc-4b62-8de9-cb25c9abc4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1]]\n",
      "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
      " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
      " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "A_prime = np.dot(np.dot(U, S), VT)\n",
    "print(A)\n",
    "print(A_prime.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6cf076-de00-4abe-82ac-32e28d87f86f",
   "metadata": {},
   "source": [
    "# 실습!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b733c643-8aaf-41f4-b363-8896f9662eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kyeul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0967b9cf-9ae4-4d1e-9fcb-954ad2688966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 수:  11314\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes')) # random_state로 매번 동일하게 shuffle한다.\n",
    "documents = dataset.data\n",
    "print('샘플 수: ', len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afdc796e-587a-4545-8991-a715b212c5b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈령용 데이터 샘플 살펴보기\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae68ae7d-bd66-4dae-9938-23a0878a16d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb4ea1aa-7404-4537-9871-0b1d44e880a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils._bunch.Bunch"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 살펴보자\n",
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e598730-548e-4f2d-b32f-ec8e36046595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed8cdd01-56e6-4b23-9c84-44912a27f2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b9f0707-6c0b-4607-9089-b25d8f3f386f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C:\\\\Users\\\\Kyeul\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\talk.politics.mideast\\\\76141',\n",
       "       'C:\\\\Users\\\\Kyeul\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\alt.atheism\\\\53281',\n",
       "       'C:\\\\Users\\\\Kyeul\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\talk.politics.mideast\\\\76350',\n",
       "       ...,\n",
       "       'C:\\\\Users\\\\Kyeul\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\rec.sport.baseball\\\\105105',\n",
       "       'C:\\\\Users\\\\Kyeul\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.sys.mac.hardware\\\\51575',\n",
       "       'C:\\\\Users\\\\Kyeul\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\rec.sport.baseball\\\\104908'],\n",
       "      dtype='<U95')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29d560dd-6edd-4679-9d76-92c11f622083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45b0396-2dc2-4e8a-9059-66e3a29475bf",
   "metadata": {},
   "source": [
    "# 전처리 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f2404a5-6355-40e7-9b4d-dded4b200ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although I realize that principle is not one o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Notwithstanding all the legitimate fuss about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, I will have to change the scoring on my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>Danny Rubenstein, an Israeli journalist, will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>\\nI agree.  Home runs off Clemens are always m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>I used HP DeskJet with Orange Micros Grappler ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>^^^^^^\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document\n",
       "0      Well i'm not sure about the story nad it did s...\n",
       "1      \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...\n",
       "2      Although I realize that principle is not one o...\n",
       "3      Notwithstanding all the legitimate fuss about ...\n",
       "4      Well, I will have to change the scoring on my ...\n",
       "...                                                  ...\n",
       "11309  Danny Rubenstein, an Israeli journalist, will ...\n",
       "11310                                                 \\n\n",
       "11311  \\nI agree.  Home runs off Clemens are always m...\n",
       "11312  I used HP DeskJet with Orange Micros Grappler ...\n",
       "11313                                        ^^^^^^\\n...\n",
       "\n",
       "[11314 rows x 1 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b3e4432-70d3-4696-a12b-8834148947a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "document    Well i'm not sure about the story nad it did s...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bb8fe92-97a3-4845-8235-38a78d60c3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Well i'm not sure about the story nad it did s...\n",
       "1        \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...\n",
       "2        Although I realize that principle is not one o...\n",
       "3        Notwithstanding all the legitimate fuss about ...\n",
       "4        Well, I will have to change the scoring on my ...\n",
       "                               ...                        \n",
       "11309    Danny Rubenstein, an Israeli journalist, will ...\n",
       "11310                                                   \\n\n",
       "11311    \\nI agree.  Home runs off Clemens are always m...\n",
       "11312    I used HP DeskJet with Orange Micros Grappler ...\n",
       "11313                                          \\nNo arg...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 특수 문자 제거 # clean_doc이라는 새로운 column에 저장\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") #오.. DataFrame에 replace로 치환할 수 있구나. 늘 re썼는데 몰랐네. #알파벳을 제외하고 모두 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"^\", \"\")\n",
    "news_df['clean_doc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4fdb24e3-d59a-4d6b-9e3d-0e98b07b8db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heelp', 'asdfasdf']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in 'heelp asdfasdf a'.split() if len(w) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04dd5ea9-c34c-49b0-9224-710dccfb3a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Well sure about story seem biased. What disagr...\n",
       "1        Yeah, expect people read FAQ, etc. actually ac...\n",
       "2        Although realize that principle your strongest...\n",
       "3        Notwithstanding legitimate fuss about this pro...\n",
       "4        Well, will have change scoring playoff pool. U...\n",
       "                               ...                        \n",
       "11309    Danny Rubenstein, Israeli journalist, will spe...\n",
       "11310                                                     \n",
       "11311    agree. Home runs Clemens always memorable. Kin...\n",
       "11312    used DeskJet with Orange Micros Grappler Syste...\n",
       "11313    argument with Murphy. scared hell when came la...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거 ) # 이걸 왜 제거하는 거지?\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))  # 낱말이 3개 미만인 것을 제거한다. \n",
    "news_df['clean_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6ca20bc-87a6-4d79-9a08-ea04978c690f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        well sure about story seem biased. what disagr...\n",
       "1        yeah, expect people read faq, etc. actually ac...\n",
       "2        although realize that principle your strongest...\n",
       "3        notwithstanding legitimate fuss about this pro...\n",
       "4        well, will have change scoring playoff pool. u...\n",
       "                               ...                        \n",
       "11309    danny rubenstein, israeli journalist, will spe...\n",
       "11310                                                     \n",
       "11311    agree. home runs clemens always memorable. kin...\n",
       "11312    used deskjet with orange micros grappler syste...\n",
       "11313    argument with murphy. scared hell when came la...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 소문자로 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
    "news_df['clean_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4079143a-fba3-4c80-9362-e80c7c66ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk로부터 불용어를 받아온다.\n",
    "stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "421166ea-ab79-477e-9fc4-f30ce2cd80e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [well, sure, about, story, seem, biased., what...\n",
       "1        [yeah,, expect, people, read, faq,, etc., actu...\n",
       "2        [although, realize, that, principle, your, str...\n",
       "3        [notwithstanding, legitimate, fuss, about, thi...\n",
       "4        [well,, will, have, change, scoring, playoff, ...\n",
       "                               ...                        \n",
       "11309    [danny, rubenstein,, israeli, journalist,, wil...\n",
       "11310                                                   []\n",
       "11311    [agree., home, runs, clemens, always, memorabl...\n",
       "11312    [used, deskjet, with, orange, micros, grappler...\n",
       "11313    [argument, with, murphy., scared, hell, when, ...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 단어로 쪼개서 토큰화함\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f86e745-e350-4e9e-87db-0d4974b30f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [well, sure, story, seem, biased., disagree, s...\n",
       "1        [yeah,, expect, people, read, faq,, etc., actu...\n",
       "2        [although, realize, principle, strongest, poin...\n",
       "3        [notwithstanding, legitimate, fuss, proposal,,...\n",
       "4        [well,, change, scoring, playoff, pool., unfor...\n",
       "                               ...                        \n",
       "11309    [danny, rubenstein,, israeli, journalist,, spe...\n",
       "11310                                                   []\n",
       "11311    [agree., home, runs, clemens, always, memorabl...\n",
       "11312    [used, deskjet, orange, micros, grappler, syst...\n",
       "11313    [argument, murphy., scared, hell, came, last, ...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd9ad621-ff17-463c-983f-773a2ada439e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yeah,',\n",
       " 'expect',\n",
       " 'people',\n",
       " 'read',\n",
       " 'faq,',\n",
       " 'etc.',\n",
       " 'actually',\n",
       " 'accept',\n",
       " 'hard',\n",
       " 'atheism?',\n",
       " 'need',\n",
       " 'little',\n",
       " 'leap',\n",
       " 'faith,',\n",
       " 'jimmy.',\n",
       " 'logic',\n",
       " 'runs',\n",
       " 'steam!',\n",
       " 'jim,',\n",
       " 'sorry',\n",
       " \"can't\",\n",
       " 'pity',\n",
       " 'you,',\n",
       " 'jim.',\n",
       " 'sorry',\n",
       " 'feelings',\n",
       " 'denial',\n",
       " 'faith',\n",
       " 'need',\n",
       " 'well,',\n",
       " 'pretend',\n",
       " 'happily',\n",
       " 'ever',\n",
       " 'anyway.',\n",
       " 'maybe',\n",
       " 'start',\n",
       " 'newsgroup,',\n",
       " 'alt.atheist.hard,',\n",
       " \"bummin'\",\n",
       " 'much?',\n",
       " 'bye-bye,',\n",
       " 'jim.',\n",
       " 'forget',\n",
       " \"flintstone's\",\n",
       " 'chewables!',\n",
       " 'bake',\n",
       " 'timmons,']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 샘플 살펴보기\n",
    "tokenized_doc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba12aebf-a824-406b-a3b1-a5c4812e6070",
   "metadata": {},
   "source": [
    "# TF-IDF 행렬 만들기\n",
    "tf-idf의 입력은 토큰이 아닌 문서이므로, 토큰화를 문서로 만드는 '역토큰화'를 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6817155c-2ce3-4eb2-b80b-eb025ed38a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역토큰화 (토큰화 작업을 역으로 되돌림)\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i]) # 토큰들을 join으로 하나의 문자열로 묶음\n",
    "    detokenized_doc.append(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4e95898e-868f-4998-9bca-07d184fc4311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"yeah, expect people read faq, etc. actually accept hard atheism? need little leap faith, jimmy. your logic runs steam! jim, sorry can't pity you, jim. sorry that have these feelings denial about faith need well, just pretend that will happily ever after anyway. maybe start newsgroup, alt.atheist.hard, won't bummin' much? bye-bye, jim. don't forget your flintstone's chewables! bake timmons,\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인\n",
    "news_df['clean_doc'][1] # 불용어가 제거되고 하나의 문서가 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "04574a96-4dc3-4f84-9725-7ea199756716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape:  (11314, 1000)\n"
     ]
    }
   ],
   "source": [
    "# 1000개의 단어로 제한된 tf-idf 행렬 생성\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000, # 상위 1000개의 단어를 보존\n",
    "                             max_df = 0.5, smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc']) #벡터화 실행\n",
    "\n",
    "# 행렬 크기 확인\n",
    "print('TF-IDF shape: ', X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892e47a-bfc6-4aad-943a-88ceb7dfda11",
   "metadata": {},
   "source": [
    "# 토픽 모델링\n",
    "이제 행렬을 분해해보자! sklean에는 Truncated SVD가 있어서 편하다. 이를 이용하면 차원 축소를 할 수 있다.  \n",
    "기존 뉴스그룹 데이터가 20개의 카테고리를 갖고 있었으므로, 20개의 토픽이 있다고 가정하고 진행하자.  \n",
    "토픽의 숫자는 n_components의 파라미터로 지정이 가능하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9eb7fd94-9bde-417a-8aaa-04bb8ad1e06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122) # 모델 생성\n",
    "svd_model.fit(X) # 학습 시작\n",
    "len(svd_model.components_) # componets_는 VT에 해당함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2e05b123-27ab-4f16-a9b5-a5ec6d3f3f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TruncatedSVD(n_components=20, n_iter=100, random_state=122)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TruncatedSVD</label><div class=\"sk-toggleable__content\"><pre>TruncatedSVD(n_components=20, n_iter=100, random_state=122)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TruncatedSVD(n_components=20, n_iter=100, random_state=122)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a320840b-979e-481d-b409-2bcb2d0429c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('just', 0.20273), ('don', 0.19953), ('like', 0.19536), ('know', 0.1882), ('people', 0.1785)]\n",
      "Topic 2: [('thanks', 0.31974), ('windows', 0.27738), ('card', 0.17369), ('drive', 0.15925), ('mail', 0.1488)]\n",
      "Topic 3: [('game', 0.32005), ('team', 0.27984), ('year', 0.26594), ('games', 0.20692), ('drive', 0.17124)]\n",
      "Topic 4: [('edu', 0.42975), ('thanks', 0.24951), ('mail', 0.17246), ('game', 0.12793), ('team', 0.12627)]\n",
      "Topic 5: [('know', 0.41951), ('does', 0.30519), ('thanks', 0.26145), ('don', 0.2107), ('just', 0.1947)]\n",
      "Topic 6: [('drive', 0.4569), ('edu', 0.21765), ('thanks', 0.18803), ('scsi', 0.15738), ('drives', 0.12184)]\n",
      "Topic 7: [('just', 0.56786), ('edu', 0.43044), ('don', 0.22504), ('like', 0.20016), ('soon', 0.0953)]\n",
      "Topic 8: [('chip', 0.2166), ('government', 0.20027), ('encryption', 0.1468), ('like', 0.14586), ('clipper', 0.14285)]\n",
      "Topic 9: [('don', 0.32342), ('know', 0.31922), ('edu', 0.28739), ('does', 0.26292), ('think', 0.20036)]\n",
      "Topic 10: [('does', 0.47787), ('card', 0.29936), ('just', 0.23814), ('video', 0.15113), ('like', 0.12301)]\n",
      "Topic 11: [('just', 0.41475), ('drive', 0.23913), ('does', 0.20627), ('file', 0.15807), ('game', 0.14768)]\n",
      "Topic 12: [('like', 0.5292), ('drive', 0.19714), ('does', 0.15332), ('file', 0.12682), ('time', 0.09455)]\n",
      "Topic 13: [('like', 0.48865), ('people', 0.40205), ('windows', 0.17163), ('israel', 0.12453), ('card', 0.12181)]\n",
      "Topic 14: [('think', 0.36399), ('just', 0.18748), ('com', 0.16075), ('like', 0.14138), ('chip', 0.12612)]\n",
      "Topic 15: [('know', 0.38757), ('00', 0.26301), ('don', 0.24181), ('sale', 0.15886), ('file', 0.14414)]\n",
      "Topic 16: [('good', 0.52033), ('people', 0.24567), ('windows', 0.23709), ('file', 0.17304), ('does', 0.16291)]\n",
      "Topic 17: [('think', 0.46302), ('space', 0.38067), ('does', 0.20833), ('israel', 0.15335), ('nasa', 0.13535)]\n",
      "Topic 18: [('com', 0.5344), ('ve', 0.4433), ('does', 0.25087), ('people', 0.14848), ('think', 0.13016)]\n",
      "Topic 19: [('space', 0.35444), ('ve', 0.25861), ('card', 0.16747), ('people', 0.16269), ('know', 0.12619)]\n",
      "Topic 20: [('ve', 0.44238), ('00', 0.24271), ('people', 0.207), ('window', 0.15947), ('does', 0.15673)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names_out() # 단어 집합. 1000개의 단어가 저장되어 있다.\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n -1 :-1]])\n",
    "\n",
    "get_topics(svd_model.components_, terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe0fa3-ba1c-41b5-b97e-0b22d4958b7f",
   "metadata": {},
   "source": [
    "LSA의 단점은 명확하다. 문서에 따라 벡더값이 변화기 때문에, 데이터가 추가되었다면 다시 처음부터 계산해야 한다.  \n",
    "상대적으로 최신인 Word2Vec은 이런 문제를 피해갈 수 있다.  \n",
    "\n",
    "그리고 토픽을 뽑는다는 의미에서 명사만 사용하여 분석하면 어떨까 싶다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e1f60-067d-42ed-b49b-8e7e978114cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
