{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a2f08c4-868c-440f-ad46-dfaa431a0908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kyeul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5755e1ef-34fe-4ad3-afdc-7979bbe8de7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 수:  11314\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes')) # random_state로 매번 동일하게 shuffle한다.\n",
    "documents = dataset.data\n",
    "print('샘플 수: ', len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a168b49f-2eb4-477c-add0-862f0fd30aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyeul\\AppData\\Local\\Temp\\ipykernel_12980\\2412442563.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") #오.. DataFrame에 replace로 치환할 수 있구나. 늘 re썼는데 몰랐네. #알파벳을 제외하고 모두 제거\n",
      "C:\\Users\\kyeul\\AppData\\Local\\Temp\\ipykernel_12980\\2412442563.py:4: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  news_df['clean_doc'] = news_df['document'].str.replace(\"^\", \"\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        [well, sure, story, seem, biased., disagree, s...\n",
       "1        [yeah,, expect, people, read, faq,, etc., actu...\n",
       "2        [although, realize, principle, strongest, poin...\n",
       "3        [notwithstanding, legitimate, fuss, proposal,,...\n",
       "4        [well,, change, scoring, playoff, pool., unfor...\n",
       "                               ...                        \n",
       "11309    [danny, rubenstein,, israeli, journalist,, spe...\n",
       "11310                                                   []\n",
       "11311    [agree., home, runs, clemens, always, memorabl...\n",
       "11312    [used, deskjet, orange, micros, grappler, syst...\n",
       "11313    [argument, murphy., scared, hell, came, last, ...\n",
       "Name: clean_doc, Length: 11314, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 특수 문자 제거 # clean_doc이라는 새로운 column에 저장\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") #오.. DataFrame에 replace로 치환할 수 있구나. 늘 re썼는데 몰랐네. #알파벳을 제외하고 모두 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"^\", \"\")\n",
    "# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거 ) # 이걸 왜 제거하는 거지?\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))  # 낱말이 3개 미만인 것을 제거한다. \n",
    "# 소문자로 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
    "# nltk로부터 불용어를 받아온다.\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 단어로 쪼개서 토큰화함\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018125ce-fb45-4223-b628-60766339582d",
   "metadata": {},
   "source": [
    "# 본격적인 LDA 예제 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f6a9e69-1106-40cd-8b6d-7f86defdab1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [well, sure, story, seem, biased., disagree, s...\n",
       "1    [yeah,, expect, people, read, faq,, etc., actu...\n",
       "2    [although, realize, principle, strongest, poin...\n",
       "3    [notwithstanding, legitimate, fuss, proposal,,...\n",
       "4    [well,, change, scoring, playoff, pool., unfor...\n",
       "Name: clean_doc, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc[:5] # 데이터를 한번 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b6301e-fea9-4ba9-a34f-e1c695fe6276",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# gensim의 corpora를 사용해서 단어 빈도를 구해보자\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m corpora\n\u001b[0;32m      3\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m copora\u001b[38;5;241m.\u001b[39mDictionary(tokenized_doc)\n\u001b[0;32m      4\u001b[0m corpus \u001b[38;5;241m=\u001b[39m [dictionary\u001b[38;5;241m.\u001b[39mdoc2bow(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tokenized_doc]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# gensim의 corpora를 사용해서 단어 빈도를 구해보자\n",
    "from gensim import corpora\n",
    "dictionary = copora.Dictionary(tokenized_doc)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_doc]\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780d208-d259-4841-bab6-175e27cc7f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
