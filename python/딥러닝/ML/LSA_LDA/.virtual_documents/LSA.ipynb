


import numpy as np


A = np.array(
    [
        [0,0,0,1,0,1,1,0,0],
        [0,0,0,1,1,0,1,0,0],
        [0,1,1,0,2,0,0,0,0],
        [1,0,0,0,0,0,0,1,1]
    ]
)
print('DTM의 크기(shape) :', np.shape(A))





# 특이값 분해
U, s, VT = np.linalg.svd(A, full_matrices=True)
print('행렬 U :')
print(U.round(2))
print('행렬 U의 크기(shape) :',np.shape(U))


print('특이값 벡터 :')
print(s.round(2))
print('특이값 벡터의 크기(shape) :',np.shape(s))





# 대각 행렬의 크기인 4 x 9의 임의의 행렬 생성
S = np.zeros((4, 9))

# 특이값을 대각행렬에 삽입
S[:4, :4] = np.diag(s) # diagonal 대각행렬을 만드는 함수

print('대각 행렬 S :')
print(S.round(2))

print('대각 행렬의 크기(shape) :')
print(np.shape(S))


np.diag(s) # 대각행렬의 값은 내림차순되어있


print('직교행렬 VT :')
print(VT.round(2))

print('직교 행렬 VT의 크기(shape) :')
print(np.shape(VT))





np.allclose(A, np.dot(np.dot(U,S), VT).round(2))





S = S[:2, :2]

print("절단된 대각 행렬 S: ")
print(S.round(2))


U = U[:, :2]
print("절단된 행렬 U :")
print(U.round(2))


VT = VT[:2, :]
print("절단된 직교행렬 VT:")
print(VT.round(2))


A_prime = np.dot(np.dot(U, S), VT)
print(A)
print(A_prime.round(2))





import pandas as pd
from sklearn.datasets import fetch_20newsgroups
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD

nltk.download('stopwords')


dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes')) # random_state로 매번 동일하게 shuffle한다.
documents = dataset.data
print('샘플 수: ', len(documents))


# 훈령용 데이터 샘플 살펴보기
documents[0]


documents[1]


# 데이터 살펴보자
type(dataset)


np.shape(dataset)


dataset.keys()


dataset.filenames


dataset.target_names





news_df = pd.DataFrame({'document':documents})
news_df


news_df.iloc[0]


# 특수 문자 제거 # clean_doc이라는 새로운 column에 저장
news_df['clean_doc'] = news_df['document'].str.replace("[^a-zA-Z]", " ") #오.. DataFrame에 replace로 치환할 수 있구나. 늘 re썼는데 몰랐네. #알파벳을 제외하고 모두 제거
news_df['clean_doc'] = news_df['document'].str.replace("^", "")
news_df['clean_doc']



[w for w in 'heelp asdfasdf a'.split() if len(w) > 3]


# 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거 ) # 이걸 왜 제거하는 거지?
news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))  # 낱말이 3개 미만인 것을 제거한다. 
news_df['clean_doc']


# 소문자로 변환
news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())
news_df['clean_doc']


# nltk로부터 불용어를 받아온다.
stop_words = stopwords.words('english')



tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 단어로 쪼개서 토큰화함
tokenized_doc


tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])
tokenized_doc


# 샘플 살펴보기
tokenized_doc[1]





# 역토큰화 (토큰화 작업을 역으로 되돌림)
detokenized_doc = []
for i in range(len(news_df)):
    t = ' '.join(tokenized_doc[i]) # 토큰들을 join으로 하나의 문자열로 묶음
    detokenized_doc.append(t) 


# 확인
news_df['clean_doc'][1] # 불용어가 제거되고 하나의 문서가 되었다.


# 1000개의 단어로 제한된 tf-idf 행렬 생성
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000, # 상위 1000개의 단어를 보존
                             max_df = 0.5, smooth_idf=True)

X = vectorizer.fit_transform(news_df['clean_doc']) #벡터화 실행

# 행렬 크기 확인
print('TF-IDF shape: ', X.shape)






svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122) # 모델 생성
svd_model.fit(X) # 학습 시작
len(svd_model.components_) # componets_는 VT에 해당함


svd_model


terms = vectorizer.get_feature_names_out() # 단어 집합. 1000개의 단어가 저장되어 있다.

def get_topics(components, feature_names, n=5):
    for idx, topic in enumerate(components):
        print("Topic %d:" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n -1 :-1]])

get_topics(svd_model.components_, terms)







