{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238735a5-3a4d-4326-810e-bff38ba326b1",
   "metadata": {},
   "source": [
    "This code come from [here](https://teddylee777.github.io/huggingface/hugginface-bbc-bert/).\n",
    "\n",
    "additional steps  \n",
    "1. analysis code \n",
    "2. add wandb code\n",
    "3. convert into vscode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729aa156-255c-46ff-9e8f-b0bed0811da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972753fd-c526-4bd5-bebb-c390a6b52aa3",
   "metadata": {},
   "source": [
    "## SEED 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde7e9a3-1155-479f-b043-8d5f0e8153bb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bdb8cc0-d050-4475-b45a-82cae01b6873",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True #?\n",
    "    torch.backends.cudnn.benchmark = True #?\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb22b4-163a-4b1e-92d0-0ad74680ac11",
   "metadata": {},
   "source": [
    "## 샘플 예제 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76e5d26a-0624-4c66-b8f7-0b771f605665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fb77b38-1c40-4141-983a-192e4ce55b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('bbc-text.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b44bd-fdca-431c-ba29-d017876f9ad3",
   "metadata": {},
   "source": [
    "# Set HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f8cd48e-a725-4b54-84d9-c0f9b3047812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyeul\\anaconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "PLM = \"abhishek/autonlp-bbc-news-classification-37229289\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PLM)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(PLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c8592b-92d3-4f7d-b856-ad2df162a5b6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2694, 2925, 1999, 1996, 2398, 1997, 7193, 2007, 2188, 3004, 3001, 12123, 2152, 1011, 6210, 2694, 2015, 1998, 3617, 2678, 14520, 2015, 3048, 2046, 1996, 2542, 2282, 1996, 2126, 2111, 3422, 2694, 2097, 2022, 25796, 2367, 1999, 2274, 2086, 2051, 1012, 2008, 2003, 2429, 2000, 2019, 6739, 5997, 2029, 5935, 2012, 1996, 3296, 7325, 8139, 2265, 1999, 5869, 7136, 2000, 6848, 2129, 2122, 2047, 6786, 2097, 4254, 2028, 1997, 2256, 8837, 2627, 14428, 2015, 1012, 2007, 1996, 2149, 2877, 1996, 9874, 8497, 1998, 2060, 4180, 2097, 2022, 5359, 2000, 7193, 3081, 2188, 6125, 2083, 5830, 5871, 18126, 2015, 3316, 1998, 19595, 2326, 11670, 2000, 2392, 4734, 1998, 12109, 5733, 1012, 2028, 1997, 1996, 2087, 5720, 1011, 2055, 6786, 1997, 8292, 2015, 2038, 2042, 3617, 1998, 3167, 2678, 14520, 2015, 1006, 1040, 19716, 1998, 26189, 2099, 1007, 1012, 2122, 2275, 1011, 2327, 8378, 2066, 1996, 2149, 1055, 14841, 6767, 1998, 1996, 2866, 1055, 3712, 1009, 2291, 3499, 2111, 2000, 2501, 3573, 2377, 8724, 1998, 2830, 3612, 2694, 8497, 2043, 2027, 2215, 1012, 7687, 1996, 2974, 4473, 2005, 2172, 2062, 3167, 5084, 2694, 1012, 2027, 2024, 2036, 2108, 2328, 1011, 1999, 2000, 2152, 1011, 6210, 2694, 4520, 2029, 2024, 2502, 2449, 1999, 2900, 1998, 1996, 2149, 2021, 12430, 2000, 2202, 2125, 1999, 2885, 2138, 1997, 1996, 3768, 1997, 2152, 1011, 6210, 4730, 1012, 2025, 2069, 2064, 2111, 2830, 3612, 2083, 4748, 16874, 2015, 2027, 2064, 2036, 5293, 2055, 11113, 28173, 3070, 2011, 2897, 1998, 3149, 20283, 5128, 2362, 2037, 2219, 1037, 1011, 2474, 1011, 11122, 2063, 4024, 1012, 2021, 2070, 2149, 6125, 1998, 5830, 1998, 5871, 3316, 2024, 5191, 2055, 2054, 2009, 2965, 2005, 2068, 1999, 3408, 1997, 6475, 12594, 2004, 2092, 2004, 4435, 4767, 1998, 13972, 9721, 2000, 6833, 1012, 2348, 1996, 2149, 5260, 1999, 2023, 2974, 2012, 1996, 2617, 2009, 2003, 2036, 1037, 5142, 2008, 2003, 2108, 2992, 1999, 2885, 3391, 2007, 1996, 3652, 2039, 15166, 1997, 2578, 2066, 3712, 1009, 1012, 2054, 6433, 2182, 2651, 2057, 2097, 2156, 1999, 3157, 2706, 2000, 1037, 2086, 2051, 1999, 1996, 2866, 4205, 20368, 1996, 4035, 3743, 1055, 11865, 20689, 8662, 2409, 1996, 4035, 2739, 4037, 1012, 2005, 1996, 7777, 1997, 1996, 4035, 2045, 2024, 2053, 3314, 1997, 2439, 6475, 6599, 2664, 1012, 2009, 2003, 1037, 2062, 7827, 3277, 2012, 1996, 2617, 2005, 3293, 2866, 18706, 2021, 4435, 9721, 2003, 2590, 2005, 3071, 1012, 2057, 2097, 2022, 3331, 2062, 2055, 4180, 9639, 2738, 2084, 2897, 9639, 2056, 5199, 7658, 7811, 2013, 4435, 4806, 3813, 2732, 9006, 2865, 6961, 2102, 1012, 1996, 4507, 2003, 2008, 2007, 19595, 7264, 10334, 2064, 2022, 1996, 3135, 1997, 4180, 1012, 2002, 2794, 1024, 1996, 4119, 2085, 2003, 2008, 2009, 2003, 2524, 2000, 5326, 1037, 4746, 2007, 2061, 2172, 3601, 1012, 2054, 2023, 2965, 2056, 19997, 8183, 19666, 2050, 3026, 3580, 2343, 1997, 2694, 5009, 2694, 2177, 2003, 2008, 1996, 2126, 2111, 2424, 1996, 4180, 2027, 2215, 2000, 3422, 2038, 2000, 2022, 11038, 2005, 2694, 7193, 1012, 2009, 2965, 2008, 6125, 1999, 2149, 3408, 2030, 6833, 2071, 2202, 1037, 7053, 2041, 1997, 8224, 1055, 2338, 1998, 2022, 1996, 3945, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = tokenizer(df['text'].iloc[0], padding=True, truncation=True)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69af6c8f-97c2-4c3a-aac1-82fe720a3b81",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2694, 2925, 1999, 1996, 2398, 1997, 7193, 2007, 2188, 3004, 3001, 12123, 2152, 1011, 6210, 2694, 2015, 1998, 3617, 2678, 14520, 2015, 3048, 2046, 1996, 2542, 2282, 1996, 2126, 2111, 3422, 2694, 2097, 2022, 25796, 2367, 1999, 2274, 2086, 2051, 1012, 2008, 2003, 2429, 2000, 2019, 6739, 5997, 2029, 5935, 2012, 1996, 3296, 7325, 8139, 2265, 1999, 5869, 7136, 2000, 6848, 2129, 2122, 2047, 6786, 2097, 4254, 2028, 1997, 2256, 8837, 2627, 14428, 2015, 1012, 2007, 1996, 2149, 2877, 1996, 9874, 8497, 1998, 2060, 4180, 2097, 2022, 5359, 2000, 7193, 3081, 2188, 6125, 2083, 5830, 5871, 18126, 2015, 3316, 1998, 19595, 2326, 11670, 2000, 2392, 4734, 1998, 12109, 5733, 1012, 2028, 1997, 1996, 2087, 5720, 1011, 2055, 6786, 1997, 8292, 2015, 2038, 2042, 3617, 1998, 3167, 2678, 14520, 2015, 1006, 1040, 19716, 1998, 26189, 2099, 1007, 1012, 2122, 2275, 1011, 2327, 8378, 2066, 1996, 2149, 1055, 14841, 6767, 1998, 1996, 2866, 1055, 3712, 1009, 2291, 3499, 2111, 2000, 2501, 3573, 2377, 8724, 1998, 2830, 3612, 2694, 8497, 2043, 2027, 2215, 1012, 7687, 1996, 2974, 4473, 2005, 2172, 2062, 3167, 5084, 2694, 1012, 2027, 2024, 2036, 2108, 2328, 1011, 1999, 2000, 2152, 1011, 6210, 2694, 4520, 2029, 2024, 2502, 2449, 1999, 2900, 1998, 1996, 2149, 2021, 12430, 2000, 2202, 2125, 1999, 2885, 2138, 1997, 1996, 3768, 1997, 2152, 1011, 6210, 4730, 1012, 2025, 2069, 2064, 2111, 2830, 3612, 2083, 4748, 16874, 2015, 2027, 2064, 2036, 5293, 2055, 11113, 28173, 3070, 2011, 2897, 1998, 3149, 20283, 5128, 2362, 2037, 2219, 1037, 1011, 2474, 1011, 11122, 2063, 4024, 1012, 2021, 2070, 2149, 6125, 1998, 5830, 1998, 5871, 3316, 2024, 5191, 2055, 2054, 2009, 2965, 2005, 2068, 1999, 3408, 1997, 6475, 12594, 2004, 2092, 2004, 4435, 4767, 1998, 13972, 9721, 2000, 6833, 1012, 2348, 1996, 2149, 5260, 1999, 2023, 2974, 2012, 1996, 2617, 2009, 2003, 2036, 1037, 5142, 2008, 2003, 2108, 2992, 1999, 2885, 3391, 2007, 1996, 3652, 2039, 15166, 1997, 2578, 2066, 3712, 1009, 1012, 2054, 6433, 2182, 2651, 2057, 2097, 2156, 1999, 3157, 2706, 2000, 1037, 2086, 2051, 1999, 1996, 2866, 4205, 20368, 1996, 4035, 3743, 1055, 11865, 20689, 8662, 2409, 1996, 4035, 2739, 4037, 1012, 2005, 1996, 7777, 1997, 1996, 4035, 2045, 2024, 2053, 3314, 1997, 2439, 6475, 6599, 2664, 1012, 2009, 2003, 1037, 2062, 7827, 3277, 2012, 1996, 2617, 2005, 3293, 2866, 18706, 2021, 4435, 9721, 2003, 2590, 2005, 3071, 1012, 2057, 2097, 2022, 3331, 2062, 2055, 4180, 9639, 2738, 2084, 2897, 9639, 2056, 5199, 7658, 7811, 2013, 4435, 4806, 3813, 2732, 9006, 2865, 6961, 2102, 1012, 1996, 4507, 2003, 2008, 2007, 19595, 7264, 10334, 2064, 2022, 1996, 3135, 1997, 4180, 1012, 2002, 2794, 1024, 1996, 4119, 2085, 2003, 2008, 2009, 2003, 2524, 2000, 5326, 1037, 4746, 2007, 2061, 2172, 3601, 1012, 2054, 2023, 2965, 2056, 19997, 8183, 19666, 2050, 3026, 3580, 2343, 1997, 2694, 5009, 2694, 2177, 2003, 2008, 1996, 2126, 2111, 2424, 1996, 4180, 2027, 2215, 2000, 3422, 2038, 2000, 2022, 11038, 2005, 2694, 7193, 1012, 2009, 2965, 2008, 6125, 1999, 2149, 3408, 2030, 6833, 2071, 2202, 1037, 7053, 2041, 1997, 8224, 1055, 2338, 1998, 2022, 1996, 3945, 102]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenized['input_ids']\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "972caed7-11a4-4f20-8b34-769c850a9623",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'tv', 'future', 'in', 'the', 'hands', 'of', 'viewers', 'with', 'home', 'theatre', 'systems', 'plasma', 'high', '-', 'definition', 'tv', '##s', 'and', 'digital', 'video', 'recorder', '##s', 'moving', 'into', 'the', 'living', 'room', 'the', 'way', 'people', 'watch', 'tv', 'will', 'be', 'radically', 'different', 'in', 'five', 'years', 'time', '.', 'that', 'is', 'according', 'to', 'an', 'expert', 'panel', 'which', 'gathered', 'at', 'the', 'annual', 'consumer', 'electronics', 'show', 'in', 'las', 'vegas', 'to', 'discuss', 'how', 'these', 'new', 'technologies', 'will', 'impact', 'one', 'of', 'our', 'favourite', 'past', '##ime', '##s', '.', 'with', 'the', 'us', 'leading', 'the', 'trend', 'programmes', 'and', 'other', 'content', 'will', 'be', 'delivered', 'to', 'viewers', 'via', 'home', 'networks', 'through', 'cable', 'satellite', 'telecom', '##s', 'companies', 'and', 'broadband', 'service', 'providers', 'to', 'front', 'rooms', 'and', 'portable', 'devices', '.', 'one', 'of', 'the', 'most', 'talked', '-', 'about', 'technologies', 'of', 'ce', '##s', 'has', 'been', 'digital', 'and', 'personal', 'video', 'recorder', '##s', '(', 'd', '##vr', 'and', 'pv', '##r', ')', '.', 'these', 'set', '-', 'top', 'boxes', 'like', 'the', 'us', 's', 'ti', '##vo', 'and', 'the', 'uk', 's', 'sky', '+', 'system', 'allow', 'people', 'to', 'record', 'store', 'play', 'pause', 'and', 'forward', 'wind', 'tv', 'programmes', 'when', 'they', 'want', '.', 'essentially', 'the', 'technology', 'allows', 'for', 'much', 'more', 'personal', '##ised', 'tv', '.', 'they', 'are', 'also', 'being', 'built', '-', 'in', 'to', 'high', '-', 'definition', 'tv', 'sets', 'which', 'are', 'big', 'business', 'in', 'japan', 'and', 'the', 'us', 'but', 'slower', 'to', 'take', 'off', 'in', 'europe', 'because', 'of', 'the', 'lack', 'of', 'high', '-', 'definition', 'programming', '.', 'not', 'only', 'can', 'people', 'forward', 'wind', 'through', 'ad', '##vert', '##s', 'they', 'can', 'also', 'forget', 'about', 'ab', '##idi', '##ng', 'by', 'network', 'and', 'channel', 'schedules', 'putting', 'together', 'their', 'own', 'a', '-', 'la', '-', 'cart', '##e', 'entertainment', '.', 'but', 'some', 'us', 'networks', 'and', 'cable', 'and', 'satellite', 'companies', 'are', 'worried', 'about', 'what', 'it', 'means', 'for', 'them', 'in', 'terms', 'of', 'advertising', 'revenues', 'as', 'well', 'as', 'brand', 'identity', 'and', 'viewer', 'loyalty', 'to', 'channels', '.', 'although', 'the', 'us', 'leads', 'in', 'this', 'technology', 'at', 'the', 'moment', 'it', 'is', 'also', 'a', 'concern', 'that', 'is', 'being', 'raised', 'in', 'europe', 'particularly', 'with', 'the', 'growing', 'up', '##take', 'of', 'services', 'like', 'sky', '+', '.', 'what', 'happens', 'here', 'today', 'we', 'will', 'see', 'in', 'nine', 'months', 'to', 'a', 'years', 'time', 'in', 'the', 'uk', 'adam', 'hume', 'the', 'bbc', 'broadcast', 's', 'fu', '##tur', '##ologist', 'told', 'the', 'bbc', 'news', 'website', '.', 'for', 'the', 'likes', 'of', 'the', 'bbc', 'there', 'are', 'no', 'issues', 'of', 'lost', 'advertising', 'revenue', 'yet', '.', 'it', 'is', 'a', 'more', 'pressing', 'issue', 'at', 'the', 'moment', 'for', 'commercial', 'uk', 'broadcasters', 'but', 'brand', 'loyalty', 'is', 'important', 'for', 'everyone', '.', 'we', 'will', 'be', 'talking', 'more', 'about', 'content', 'brands', 'rather', 'than', 'network', 'brands', 'said', 'tim', 'han', '##lon', 'from', 'brand', 'communications', 'firm', 'star', '##com', 'media', '##ves', '##t', '.', 'the', 'reality', 'is', 'that', 'with', 'broadband', 'connections', 'anybody', 'can', 'be', 'the', 'producer', 'of', 'content', '.', 'he', 'added', ':', 'the', 'challenge', 'now', 'is', 'that', 'it', 'is', 'hard', 'to', 'promote', 'a', 'programme', 'with', 'so', 'much', 'choice', '.', 'what', 'this', 'means', 'said', 'stacey', 'jo', '##ln', '##a', 'senior', 'vice', 'president', 'of', 'tv', 'guide', 'tv', 'group', 'is', 'that', 'the', 'way', 'people', 'find', 'the', 'content', 'they', 'want', 'to', 'watch', 'has', 'to', 'be', 'simplified', 'for', 'tv', 'viewers', '.', 'it', 'means', 'that', 'networks', 'in', 'us', 'terms', 'or', 'channels', 'could', 'take', 'a', 'leaf', 'out', 'of', 'google', 's', 'book', 'and', 'be', 'the', 'search', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39478f2-d58a-4579-85f5-1c754790e7fc",
   "metadata": {},
   "source": [
    "## Label Map 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a091e853-3ac8-4ed7-afe3-7218cfe11e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    'sport':0,\n",
    "    'business': 1,\n",
    "    'politics': 2,\n",
    "    'tech':3,\n",
    "    'entertainment': 4\n",
    "}\n",
    "\n",
    "df['category_num'] = df['category'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c926ef92-e49c-41f1-a6c6-491ca3316b9f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>category_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>business</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>politics</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>politics</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>sport</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text  \\\n",
       "0              tech  tv future in the hands of viewers with home th...   \n",
       "1          business  worldcom boss  left books alone  former worldc...   \n",
       "2             sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3             sport  yeading face newcastle in fa cup premiership s...   \n",
       "4     entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "...             ...                                                ...   \n",
       "2220       business  cars pull down us retail figures us retail sal...   \n",
       "2221       politics  kilroy unveils immigration policy ex-chatshow ...   \n",
       "2222  entertainment  rem announce new glasgow concert us band rem h...   \n",
       "2223       politics  how political squabbles snowball it s become c...   \n",
       "2224          sport  souness delight at euro progress boss graeme s...   \n",
       "\n",
       "      category_num  \n",
       "0                3  \n",
       "1                1  \n",
       "2                0  \n",
       "3                0  \n",
       "4                4  \n",
       "...            ...  \n",
       "2220             1  \n",
       "2221             2  \n",
       "2222             4  \n",
       "2223             2  \n",
       "2224             0  \n",
       "\n",
       "[2225 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e1a58-0386-464d-b890-9f8e7d48bc73",
   "metadata": {},
   "source": [
    "## Split data into train_set and valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e16bd576-3311-4c1d-861e-4f10bc0eb34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf172d3b-1c27-418f-9edf-648c22b58ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['category_num'],\n",
    "                                                    stratify=df['category_num'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf4c8c2-4ae9-4fd0-9ddc-221f37a71d2a",
   "metadata": {},
   "source": [
    "## Batch Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be88e3f6-a662-4d75-8d34-39855a8d81f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e8a47b7-0c05-46c0-a741-41e6b9ac3a8f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tokenized = tokenizer(df['text'].iloc[:10].tolist(), padding=True, truncation=True)\n",
    "batch_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c64fddad-c2c9-4ef7-a767-322e17df3563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 512)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(batch_tokenized['input_ids']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e2afcd-d74f-4dfe-9f31-bf0e2c936c13",
   "metadata": {},
   "source": [
    "## Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "519a10b4-1f8b-44a4-bbc3-20db526239a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dabeef2f-b889-4c2c-82f0-7f50f58400ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        super().__init__()\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "376d762c-7f86-4c9c-8a5f-6dfd8e62b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 생성\n",
    "train_ds = CustomDataset(x_train, y_train)\n",
    "valid_ds = CustomDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb719dc6-d119-427d-91b2-7bbf5c78c7a7",
   "metadata": {},
   "source": [
    "## DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53accbb9-cda3-44b9-950f-0e4b0b8a959a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# torch 디바이스 지정\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f114ab60-deca-42b1-a691-c573ed8fce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch, tokenizer):\n",
    "    text_list, label_list = [], []\n",
    "\n",
    "    for text, label in batch:\n",
    "        text_list.append(text)\n",
    "        label_list.append(label)\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64) # tensor 정수\n",
    "\n",
    "    # padding으로 길이를 맞춤\n",
    "    text_tokenized = tokenizer(text_list, padding=True, truncation=True, return_tensors='pt') # pytorch\n",
    "\n",
    "    return text_tokenized, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83d492e7-1f98-4591-9a01-35588fd1a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=lambda x: collate_batch(x, tokenizer))\n",
    "valid_loader = DataLoader(valid_ds,\n",
    "                          batch_size=32,\n",
    "                          shuffle=False,\n",
    "                          collate_fn=lambda x: collate_batch(x, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6aa054a0-840c-4a01-b861-f14fe53f3066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader)) #?\n",
    "print(x[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d2837b6-944a-4cb3-b55d-9594666b73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8988e-e506-4c65-9b3e-50244a291077",
   "metadata": {},
   "source": [
    "# 모델 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2778cfb5-d173-4a6d-a1e3-2b13e566ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67f3272e-6a41-4241-b71a-b7880b096e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 1024)\n",
      "      (token_type_embeddings): Embedding(2, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=1024, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28d41f19-a96e-4d18-802f-4fd1fccb051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze weight\n",
    "# PLM의 weight는 업데이트 하지 않고\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False #??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "268ceb95-33c3-44cf-9e28-b69c741e136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(1024, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 32),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5588e06a-7908-4067-9986-9e6e538fd266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 변경된 classifier 가중치 업데이트 가능 여부 확인?\n",
    "# 분류기만 전이학습함\n",
    "for param in model.classifier.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d082ee93-fc05-4467-b3da-ad9eb65d81ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7513,  3084,  ...,  4358,  2012,   102],\n",
       "         [  101,  4977,  1011,  ...,     0,     0,     0],\n",
       "         [  101, 19267,  7016,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  2399,  4607,  ...,  2000,  1037,   102],\n",
       "         [  101, 21291,  3089,  ...,     0,     0,     0],\n",
       "         [  101,  5924,  7065,  ...,  2709,  2009,   102]], device='cuda:0'),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 입력의 각 키별(input_ids, token_type_ids, attention_mask)를 device에 로드\n",
    "inputs = {k: v.to(device) for k, v in x.items()}\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a5e1ace-899d-418e-8d19-bede555cf93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=32, bias=True)\n",
       "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=32, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec8f40-d9c7-44f8-ab22-e7deb643ac9d",
   "metadata": {},
   "source": [
    "## define criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f293a74-c7df-40c8-8f44-413b0c4dd9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "416bfcf6-0f42-4f30-9706-cf26eec36aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    # loss와 accuracy 계산을 위한 임시 변수\n",
    "    running_loss = 0\n",
    "    corr = 0\n",
    "    counts = 0\n",
    "\n",
    "    # Prograss Bar 출력을 위해 tqdm으로 매핑\n",
    "    prograss_bar = tqdm(data_loader, unit='batch', total=len(data_loader), mininterval=1)\n",
    "\n",
    "\n",
    "    # mini-batch 학습\n",
    "    for idx, (txt, lbl) in enumerate(prograss_bar):\n",
    "        # txt, label 데이터를 device에 올림\n",
    "        inputs = {k:v.to(device) for k, v in txt.items()}\n",
    "        lbl = lbl.to(device)\n",
    "\n",
    "        # 누적 Gradient를 초기화 함\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        output = model(**inputs)\n",
    "        # 예측값인 logits만 추출함\n",
    "        output = output.logits\n",
    "\n",
    "        # 손실 함수에 output, label을 대입하여 구함\n",
    "        loss = criterion(output, lbl)\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "\n",
    "        # update Gradient\n",
    "        optimizer.step()\n",
    "        # 확률값이 최대인 index를 구함 get Probability max index\n",
    "        output = output.argmax(dim=1)\n",
    "\n",
    "        # 정답 갯수를 구함\n",
    "        corr += (output == lbl).sum().item()\n",
    "        counts += len(lbl)\n",
    "\n",
    "        # batch별 loss 계산하여 누적합을 구함\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 프로그래스바에 학습 상황 업데이트\n",
    "        prograss_bar.set_description(f\"training loss: {running_loss/(idx+1):.5f}, training accuracy: {corr / counts:.5f}\")\n",
    "\n",
    "    # 누적된 정답수를 전체 개수로 나누어 정확도 계산\n",
    "    acc = corr / len(data_loader.dataset)\n",
    "\n",
    "\n",
    "    # 평균 손실과 정확도를 반환함\n",
    "    # train_loss, train_acc\n",
    "    return running_loss / len(data_loader), acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35515b8f-75e9-41b4-a87c-37c271a13c2e",
   "metadata": {},
   "source": [
    "### Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "597048fa-b950-41aa-beaa-23de9b6c41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(model, data_loader, criterion, device):\n",
    "    # model.eval()은 모델 평가모드\n",
    "    # evaluation 진행시 반드시 작성해줘야 함\n",
    "    model.eval()\n",
    "\n",
    "    # torch.no_grad()로 Gradient가 업데이트 되는 것을 방지함\n",
    "    with torch.no_grad():\n",
    "        corr = 0\n",
    "        running_loss = 0\n",
    "\n",
    "        # evaluate each batch data.\n",
    "        for txt, label in data_loader:\n",
    "            # upload txt and label into device.\n",
    "            inputs = {k:v.to(device) for k, v in txt.items()}\n",
    "            label = label.to(device)\n",
    "\n",
    "            output = model(**inputs)\n",
    "\n",
    "            #예측 값인 logits를 가져옴\n",
    "            output = output.logits\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            # 출력의 확률값이 가장 높은 index를 가져옴\n",
    "            output = output.argmax(dim=1)\n",
    "\n",
    "            # 맞은 갯수를 구함\n",
    "            corr += (output == label).sum().item()\n",
    "\n",
    "            # batch 별 loss를 계산\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # culculate validation's accuracy\n",
    "    acc = corr / len(data_loader.dataset)\n",
    "\n",
    "    return running_loss / len(data_loader), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efb0e67c-5ec1-4260-a2ce-821689188091",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "model_name = 'BBC-Text-CLF-BERT'\n",
    "min_loss = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bd00015-1923-42de-a1d0-9c807dedc7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.42567, training accuracy: 0.99213: 100%|███████████████████████████| 56/56 [09:26<00:00, 10.12s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] val_loss has been improved from 0.39143 to 0.31969. Saving Model.\n",
      "epoch 01, loss: 0.42567, acc: 0.99213, val_loss: 0.31969, val_accuracy:0.99326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.36583, training accuracy: 0.99213: 100%|███████████████████████████| 56/56 [08:32<00:00,  9.15s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] val_loss has been improved from 0.31969 to 0.28635. Saving Model.\n",
      "epoch 02, loss: 0.36583, acc: 0.99213, val_loss: 0.28635, val_accuracy:0.99326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.33264, training accuracy: 0.99270: 100%|███████████████████████████| 56/56 [02:55<00:00,  3.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] val_loss has been improved from 0.28635 to 0.25931. Saving Model.\n",
      "epoch 03, loss: 0.33264, acc: 0.99270, val_loss: 0.25931, val_accuracy:0.99326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.30376, training accuracy: 0.99213: 100%|███████████████████████████| 56/56 [05:32<00:00,  5.94s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] val_loss has been improved from 0.25931 to 0.23335. Saving Model.\n",
      "epoch 04, loss: 0.30376, acc: 0.99213, val_loss: 0.23335, val_accuracy:0.99326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loss: 0.27993, training accuracy: 0.99213: 100%|███████████████████████████| 56/56 [02:55<00:00,  3.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] val_loss has been improved from 0.23335 to 0.21102. Saving Model.\n",
      "epoch 05, loss: 0.27993, acc: 0.99213, val_loss: 0.21102, val_accuracy:0.99551\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = model_train(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "    val_loss, val_acc = model_evaluate(model, valid_loader, criterion, device)\n",
    "\n",
    "    if val_loss < min_loss:\n",
    "        print(f\"[Info] val_loss has been improved from {min_loss:.5f} to {val_loss:.5f}. Saving Model.\")\n",
    "        min_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'{model_name}.pth')\n",
    "\n",
    "    print(f'epoch {epoch+1:02d}, loss: {train_loss:.5f}, acc: {train_acc:.5f}, val_loss: {val_loss:.5f}, val_accuracy:{val_acc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c7a1d-9152-41e5-8609-2e40b666113b",
   "metadata": {},
   "source": [
    "# 최종 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74738cd5-be50-4b4d-bfcc-263ad95609e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best weight\n",
    "model.load_state_dict(torch.load(f'{model_name}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c860d9dc-5023-4d73-85d5-bf680a4d58d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.21102, accuracy: 0.99551\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    val_loss, val_acc = model_evaluate(model, valid_loader, criterion, device)\n",
    "    print(f\"loss: {val_loss:.5f}, accuracy: {val_acc:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd3846c-fb60-4c6c-83e7-2b7d09b27dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
