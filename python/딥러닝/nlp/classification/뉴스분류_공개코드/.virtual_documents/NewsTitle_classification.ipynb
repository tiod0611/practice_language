


import random
import numpy as np
import os

import torch





SEED = 123


def seed_everything(seed=SEED):
    random.seed(seed)
    np.random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True #?
    torch.backends.cudnn.benchmark = True #?

seed_everything(SEED)





import json
from tqdm import tqdm
import numpy as np
import pandas as pd


df = pd.read_csv('bbc-text.csv')
df.head()





from transformers import AutoTokenizer, AutoModelForSequenceClassification

PLM = "abhishek/autonlp-bbc-news-classification-37229289"

tokenizer = AutoTokenizer.from_pretrained(PLM)
model = AutoModelForSequenceClassification.from_pretrained(PLM)


tokenized = tokenizer(df['text'].iloc[0], padding=True, truncation=True)
tokenized


input_ids = tokenized['input_ids']
print(input_ids)


print(tokenizer.convert_ids_to_tokens(input_ids))





label_map = {
    'sport':0,
    'business': 1,
    'politics': 2,
    'tech':3,
    'entertainment': 4
}

df['category_num'] = df['category'].map(label_map)


df





from sklearn.model_selection import train_test_split


x_train, x_test, y_train, y_test = train_test_split(df['text'], df['category_num'],
                                                    stratify=df['category_num'],
                                                    test_size=0.2,
                                                    random_state=SEED)





tokenizer.model_max_length


batch_tokenized = tokenizer(df['text'].iloc[:10].tolist(), padding=True, truncation=True)
batch_tokenized[0]


np.array(batch_tokenized['input_ids']).shape





from torch.utils.data import DataLoader, Dataset
from torchtext.vocab import build_vocab_from_iterator


class CustomDataset(Dataset):
    def __init__(self, texts, labels):
        super().__init__()
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.labels)


    def __getitem__(self, idx):
        text = self.texts.iloc[idx]
        label = self.labels.iloc[idx]

        return text, label


# Dataset 생성
train_ds = CustomDataset(x_train, y_train)
valid_ds = CustomDataset(x_test, y_test)





import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence

# torch 디바이스 지정
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(device)


def collate_batch(batch, tokenizer):
    text_list, label_list = [], []

    for text, label in batch:
        text_list.append(text)
        label_list.append(label)

    label_list = torch.tensor(label_list, dtype=torch.int64) # tensor 정수

    # padding으로 길이를 맞춤
    text_tokenized = tokenizer(text_list, padding=True, truncation=True, return_tensors='pt') # pytorch

    return text_tokenized, label_list


train_loader = DataLoader(train_ds,
                          batch_size=128,
                          shuffle=True,
                          collate_fn=lambda x: collate_batch(x, tokenizer))
valid_loader = DataLoader(valid_ds,
                          batch_size=128,
                          shuffle=False,
                          collate_fn=lambda x: collate_batch(x, tokenizer))


x, y = next(iter(train_loader)) #?
print(x[0])
print(y[0])


x, y = x.to(device), y.to(device)





from tqdm import tqdm
import torch.optim as optim


print(model)


# freeze weight
# PLM의 weight는 업데이트 하지 않고
for param in model.parameters():
    param.requires_grad=False #??



model.classifier = nn.Sequential(
    nn.Linear(1024, 256),
    nn.BatchNorm1d(256),
    nn.ReLU(),
    nn.Linear(256, 32),
    nn.BatchNorm1d(32),
    nn.ReLU(),
    nn.Linear(32, 5)
)


# 변경된 classifier 가중치 업데이트 가능 여부 확인?
# 분류기만 전이학습함
for param in model.classifier.parameters():
    print(param.requires_grad)


# 입력의 각 키별(input_ids, token_type_ids, attention_mask)를 device에 로드
inputs = {k: v.to(device) for k, v in x.items()}
inputs


model.to(device)





criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)


def model_train(model, data_loader, criterion, optimizer, device):
    model.train()

    # loss와 accuracy 계산을 위한 임시 변수
    running_loss = 0
    corr = 0
    counts = 0

    # Prograss Bar 출력을 위해 tqdm으로 매핑
    prograss_bar = tqdm(data_loader, unit='batch', total=len(data_loader), mininterval=1)


    # mini-batch 학습
    for idx, (text, lbl) in enumerate(prograss_bar):
        # txt, label 데이터를 device에 올림
        inputs = {k:v.to(device) for k, v in txt.items()}
        lbl = lbl.to(device)

        # 누적 Gradient를 초기화 함
        optimizer.zero_grad()

        # Forward
        output = model(**input)
        # 예측값인 logits만 추출함
        output = output.logits

        # 손실 함수에 output, label을 대입하여 구함
        loss = criterion(output, lbl)
        # Backward
        loss.backward()

        # update Gradient
        optimizer.step()
        # 확률값이 최대인 index를 구함 get Probability max index
        output = output.argmax(dim=1)

        # 정답 갯수를 구함
        corr += (output == lbl).sum().item()
        counts += len(lbl)

        # batch별 loss 계산하여 누적합을 구함
        running_loss += loss.item()

        # 프로그래스바에 학습 상황 업데이트
        prograss_bar.set_description(f"training loss: {running_loss/(idx+1):.5f}, training accuracy: {corr / counts:.5f}")

    # 누적된 정답수를 전체 개수로 나누어 정확도 계산
    acc = corr / len(data_loader.dataset)


    # 평균 손실과 정확도를 반환함
    # train_loss, train_acc
    return running_loss / len(data_loader), acc






