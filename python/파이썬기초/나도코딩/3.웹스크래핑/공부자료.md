## Selenium
동적 페이지 크롤링
### 2-1 (구글 무비)
+ soup.prettify() -> html 형식에 맞춰서 예쁘게 출력함.
+ (중요) 접속하는 header 정보에 따라 웹사이트의 정보가 다르게 나옴.
+ bs에는 xpath를 특정 하위 태그만 가져오기 위해서는

### 2-2 페이지 자동 스크롤 다운

driver.execute_script(f"window.scrollTo(0, {document_height_before + scroll_height});")
=> window.scrollTo(x, y)는 스크롤을 이동하는 script
=> 여러 사이트에서 scroll down으로 document.body.scrollHeight을 사용하지만, 위 코드처럼 지금 높이에서 특정 값을 더하는 식으로 갱신하는 것이 안정성이 있었다. (youtube 경우)


### 2-3 페이지 다운된 브라우저 크롤링
+ selenium으로 동적 페이지에 접근했다면 soup 객체를 만들 때, requests를 쓰지 말고 driver.page_source로 soup 객체를 만들 수 있다.
+ soup.\[find_all() | find()]에서 attrs를 쓸 때 요소의 이름을 대괄호\[]로 감싼다면 해당 이름에 해당하는 것을 찾게 된다. 
	+ ex) soup.find('div', attrs={'class':\['hi', 'hello']}) => class가 hi 이거나 hello 인 div 테그를 찾음